{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Models in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Machine learning models can be implemented in tensorflow using the lower level tensorflow primitives which are operations and tensors.\n",
    "\n",
    "Tensorflow also provides low level primitives to specify optimizers that can find the maxima or minima of a loss function.\n",
    "\n",
    "If a machine learning model can be reduced to linear algebraic operations, it can be implemented in tensorflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will see:\n",
    "\n",
    "1. Linear regression\n",
    "\n",
    "2. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "rng = np.random\n",
    "logs_path = '../../logs/lesson2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the others are considered to be dependent variables. For our example, we want to relate the variable Y to the variable X using a linear regression model. \n",
    "\n",
    "Specification of the model:\n",
    "$y$ = $b$ + $w_1$$x_1$ + ... +  $w_p$$x_p$\n",
    "- $y$ is the regressed variable\n",
    "- $w$'s are the weights\n",
    "- $b$ is the bias term\n",
    "- $x$'s are the features used to model y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Some toy data\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First we will build the computational graph for linear regression based on the algebraic equation that the model is defined by. We will use two new TensorFlow concepts, placeholders and variables, to build our graph. \n",
    "\n",
    "Placeholders are entry points into the graph allowing for training data to be passed into the graph.\n",
    "\n",
    "Variables are used to represent parameters of the graph which need to retain their value between runs (iterations) while training in a session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "1. Placeholders: https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "2. Variables: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To quote [TensorFlow's programmer's guide](https://www.tensorflow.org/programmers_guide/variables):\n",
    ">A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    ">Variables are manipulated via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    ">Internally, a tf.Variable stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the computational graph for linear regression with 1 explanatory variable\n",
    "# p = 1\n",
    "\n",
    "# Input to the graph\n",
    "y = tf.placeholder(dtype = tf.float32, name = 'InputData') # Placeholders - https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "x = tf.placeholder(dtype = tf.float32, name = 'LabelData')\n",
    "\n",
    "# Model parameters are defined using variables\n",
    "# Variables - https://www.tensorflow.org/programmers_guide/variables\n",
    "# Variables retain their value even outside the bounds of a session's run call\n",
    "w = tf.Variable(initial_value = rng.randn(), name = \"weight\") \n",
    "b = tf.Variable(initial_value = rng.randn(), name = \"bias\")\n",
    "\n",
    "# Connecting up the nodes in our linear model\n",
    "# y = b + Wx\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.add(b, tf.multiply(w, x))\n",
    "\n",
    "# prediction holds the tensor that is the output of the operation add which takes tensors b, and the output of the multiply operation between the weight w, and the input x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model is complete, but our computational graph is not yet complete. To complete the computational graph, we need to define a loss function and an optimization strategy to allow for the training of the free variables, $b$ and $w$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Tensorflow provides various inbuilt optimizers that allow for the optimization of objective functions. These inbuilt optimizers are mostly directed toward neural network optimization, but a user can specify their own optimization functions by extending a base class. The base class provides access to various methods that calculate the gradients at all points in our computational graph. However, for most industrial projects the set of optimizers provided by TensorFlow are sufficient. \n",
    "\n",
    "To optimize a linear regressor, we will use the inbuilt Gradient Descent Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "3. reduce_sum operation: https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "4. Gradient descent optimizer: https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss for our model\n",
    "# Loss is the mean squared error between actual $y$ and predicted $y$\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_sum( input_tensor = tf.pow(prediction-y, 2))/(2*n_samples)\n",
    "# reduce_sum is a function to compute the sum across dimensions of a tensor. In this case, the input tensor is a 1 x n_samples dimensional tensor of the prediction errors corresponding to the training samples  \n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "#Our previous definitions implicitly creates the relation between the loss and the variables w and b \n",
    "\n",
    "# We can use gradient descent to train our linear model\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now have a complete computational graph. Each run of the optimizer takes one sample of X and Y as input, makes a prediction. The optimizer updates the free variables in its loss function based on the prediction for that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We also need an operation to initialize our global variables (w and b)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss= 48.691528320 w= 1.9161016 b= -0.5114414\n",
      "Epoch: 0002 loss= 19.436344147 w= 1.378653 b= -0.58614177\n",
      "Epoch: 0003 loss= 7.820686817 w= 1.0395125 b= -0.63258046\n",
      "Epoch: 0004 loss= 3.214914083 w= 0.8254732 b= -0.66119117\n",
      "Epoch: 0005 loss= 1.392511606 w= 0.69035304 b= -0.67855614\n",
      "Epoch: 0006 loss= 0.673821211 w= 0.60501873 b= -0.6888277\n",
      "Epoch: 0007 loss= 0.391865104 w= 0.5510915 b= -0.69462514\n",
      "Epoch: 0008 loss= 0.282136917 w= 0.51697713 b= -0.6976011\n",
      "Epoch: 0009 loss= 0.239955932 w= 0.49536166 b= -0.6987978\n",
      "Epoch: 0010 loss= 0.224030703 w= 0.48163122 b= -0.69887304\n",
      "Epoch: 0011 loss= 0.218160123 w= 0.47287512 b= -0.69824153\n",
      "Epoch: 0012 loss= 0.216042563 w= 0.4672572 b= -0.6971652\n",
      "Epoch: 0013 loss= 0.215262309 w= 0.46361905 b= -0.695809\n",
      "Epoch: 0014 loss= 0.214915827 w= 0.46122992 b= -0.6942771\n",
      "Epoch: 0015 loss= 0.214680210 w= 0.45962885 b= -0.69263524\n",
      "Epoch: 0016 loss= 0.214449912 w= 0.458525 b= -0.6909246\n",
      "Epoch: 0017 loss= 0.214197412 w= 0.457735 b= -0.6891717\n",
      "Epoch: 0018 loss= 0.213921100 w= 0.45714304 b= -0.6873928\n",
      "Epoch: 0019 loss= 0.213625804 w= 0.4566762 b= -0.68559825\n",
      "Epoch: 0020 loss= 0.213317409 w= 0.45628843 b= -0.68379474\n",
      "Epoch: 0021 loss= 0.213000298 w= 0.45595053 b= -0.68198645\n",
      "Epoch: 0022 loss= 0.212677792 w= 0.4556442 b= -0.6801759\n",
      "Epoch: 0023 loss= 0.212352037 w= 0.45535803 b= -0.6783649\n",
      "Epoch: 0024 loss= 0.212024525 w= 0.45508465 b= -0.6765542\n",
      "Epoch: 0025 loss= 0.211696163 w= 0.45481935 b= -0.67474467\n",
      "Epoch: 0026 loss= 0.211367577 w= 0.45455933 b= -0.6729367\n",
      "Epoch: 0027 loss= 0.211039171 w= 0.45430285 b= -0.67113036\n",
      "Epoch: 0028 loss= 0.210711122 w= 0.4540486 b= -0.669326\n",
      "Epoch: 0029 loss= 0.210383624 w= 0.45379588 b= -0.6675236\n",
      "Epoch: 0030 loss= 0.210056782 w= 0.45354423 b= -0.6657235\n",
      "Epoch: 0031 loss= 0.209730580 w= 0.4532934 b= -0.66392547\n",
      "Epoch: 0032 loss= 0.209405184 w= 0.45304328 b= -0.66212964\n",
      "Epoch: 0033 loss= 0.209080473 w= 0.45279366 b= -0.6603359\n",
      "Epoch: 0034 loss= 0.208756581 w= 0.45254442 b= -0.6585443\n",
      "Epoch: 0035 loss= 0.208433464 w= 0.4522955 b= -0.656755\n",
      "Epoch: 0036 loss= 0.208111048 w= 0.45204702 b= -0.6549677\n",
      "Epoch: 0037 loss= 0.207789496 w= 0.4517988 b= -0.6531826\n",
      "Epoch: 0038 loss= 0.207468733 w= 0.4515509 b= -0.6513998\n",
      "Epoch: 0039 loss= 0.207148716 w= 0.4513034 b= -0.64961916\n",
      "Epoch: 0040 loss= 0.206829503 w= 0.45105618 b= -0.6478407\n",
      "Epoch: 0041 loss= 0.206511065 w= 0.45080924 b= -0.64606434\n",
      "Epoch: 0042 loss= 0.206193402 w= 0.45056263 b= -0.6442902\n",
      "Epoch: 0043 loss= 0.205876529 w= 0.45031628 b= -0.6425183\n",
      "Epoch: 0044 loss= 0.205560401 w= 0.45007032 b= -0.64074844\n",
      "Epoch: 0045 loss= 0.205245048 w= 0.4498246 b= -0.63898087\n",
      "Epoch: 0046 loss= 0.204930544 w= 0.44957915 b= -0.63721544\n",
      "Epoch: 0047 loss= 0.204616755 w= 0.44933403 b= -0.63545215\n",
      "Epoch: 0048 loss= 0.204303756 w= 0.44908923 b= -0.633691\n",
      "Epoch: 0049 loss= 0.203991488 w= 0.4488447 b= -0.63193214\n",
      "Epoch: 0050 loss= 0.203680009 w= 0.4486005 b= -0.6301753\n",
      "Epoch: 0051 loss= 0.203369305 w= 0.4483566 b= -0.6284206\n",
      "Epoch: 0052 loss= 0.203059316 w= 0.448113 b= -0.6266681\n",
      "Epoch: 0053 loss= 0.202750117 w= 0.44786966 b= -0.6249178\n",
      "Epoch: 0054 loss= 0.202441722 w= 0.4476266 b= -0.6231697\n",
      "Epoch: 0055 loss= 0.202134058 w= 0.4473839 b= -0.62142366\n",
      "Epoch: 0056 loss= 0.201827124 w= 0.44714153 b= -0.61967963\n",
      "Epoch: 0057 loss= 0.201520905 w= 0.44689947 b= -0.61793786\n",
      "Epoch: 0058 loss= 0.201215491 w= 0.44665763 b= -0.6161982\n",
      "Epoch: 0059 loss= 0.200910822 w= 0.4464161 b= -0.61446077\n",
      "Epoch: 0060 loss= 0.200606927 w= 0.4461749 b= -0.61272544\n",
      "Epoch: 0061 loss= 0.200303689 w= 0.445934 b= -0.61099225\n",
      "Epoch: 0062 loss= 0.200001314 w= 0.44569337 b= -0.6092611\n",
      "Epoch: 0063 loss= 0.199699596 w= 0.44545302 b= -0.60753214\n",
      "Epoch: 0064 loss= 0.199398607 w= 0.44521302 b= -0.6058052\n",
      "Epoch: 0065 loss= 0.199098393 w= 0.4449733 b= -0.6040805\n",
      "Epoch: 0066 loss= 0.198798954 w= 0.4447338 b= -0.6023579\n",
      "Epoch: 0067 loss= 0.198500171 w= 0.4444946 b= -0.6006373\n",
      "Epoch: 0068 loss= 0.198202133 w= 0.44425577 b= -0.59891886\n",
      "Epoch: 0069 loss= 0.197904870 w= 0.4440171 b= -0.5972025\n",
      "Epoch: 0070 loss= 0.197608292 w= 0.4437788 b= -0.59548825\n",
      "Epoch: 0071 loss= 0.197312489 w= 0.44354078 b= -0.5937762\n",
      "Epoch: 0072 loss= 0.197017342 w= 0.4433031 b= -0.5920662\n",
      "Epoch: 0073 loss= 0.196722984 w= 0.44306573 b= -0.59035826\n",
      "Epoch: 0074 loss= 0.196429327 w= 0.4428286 b= -0.5886525\n",
      "Epoch: 0075 loss= 0.196136415 w= 0.44259173 b= -0.5869488\n",
      "Epoch: 0076 loss= 0.195844188 w= 0.44235525 b= -0.5852472\n",
      "Epoch: 0077 loss= 0.195552692 w= 0.442119 b= -0.5835477\n",
      "Epoch: 0078 loss= 0.195261881 w= 0.4418831 b= -0.58185035\n",
      "Epoch: 0079 loss= 0.194971859 w= 0.44164744 b= -0.5801551\n",
      "Epoch: 0080 loss= 0.194682494 w= 0.4414121 b= -0.5784618\n",
      "Epoch: 0081 loss= 0.194393843 w= 0.44117695 b= -0.5767707\n",
      "Epoch: 0082 loss= 0.194105923 w= 0.44094214 b= -0.5750816\n",
      "Epoch: 0083 loss= 0.193818659 w= 0.44070765 b= -0.5733945\n",
      "Epoch: 0084 loss= 0.193532154 w= 0.44047338 b= -0.5717096\n",
      "Epoch: 0085 loss= 0.193246320 w= 0.44023946 b= -0.5700267\n",
      "Epoch: 0086 loss= 0.192961186 w= 0.44000584 b= -0.56834584\n",
      "Epoch: 0087 loss= 0.192676738 w= 0.4397725 b= -0.56666714\n",
      "Epoch: 0088 loss= 0.192393035 w= 0.4395394 b= -0.56499046\n",
      "Epoch: 0089 loss= 0.192110002 w= 0.43930662 b= -0.56331587\n",
      "Epoch: 0090 loss= 0.191827670 w= 0.43907413 b= -0.56164324\n",
      "Epoch: 0091 loss= 0.191546068 w= 0.4388419 b= -0.55997276\n",
      "Epoch: 0092 loss= 0.191265047 w= 0.43860993 b= -0.558304\n",
      "Epoch: 0093 loss= 0.190984756 w= 0.43837824 b= -0.5566374\n",
      "Epoch: 0094 loss= 0.190705135 w= 0.4381469 b= -0.55497295\n",
      "Epoch: 0095 loss= 0.190426290 w= 0.4379158 b= -0.55331063\n",
      "Epoch: 0096 loss= 0.190148041 w= 0.437685 b= -0.5516503\n",
      "Epoch: 0097 loss= 0.189870477 w= 0.43745446 b= -0.5499919\n",
      "Epoch: 0098 loss= 0.189593643 w= 0.43722424 b= -0.5483355\n",
      "Epoch: 0099 loss= 0.189317420 w= 0.43699428 b= -0.5466813\n",
      "Epoch: 0100 loss= 0.189041957 w= 0.43676466 b= -0.545029\n",
      "Epoch: 0101 loss= 0.188767105 w= 0.43653533 b= -0.5433788\n",
      "Epoch: 0102 loss= 0.188492924 w= 0.4363062 b= -0.54173046\n",
      "Epoch: 0103 loss= 0.188219488 w= 0.43607742 b= -0.5400844\n",
      "Epoch: 0104 loss= 0.187946677 w= 0.43584883 b= -0.5384403\n",
      "Epoch: 0105 loss= 0.187674552 w= 0.43562055 b= -0.5367981\n",
      "Epoch: 0106 loss= 0.187403053 w= 0.43539256 b= -0.53515786\n",
      "Epoch: 0107 loss= 0.187132239 w= 0.43516484 b= -0.53351974\n",
      "Epoch: 0108 loss= 0.186862066 w= 0.43493742 b= -0.5318836\n",
      "Epoch: 0109 loss= 0.186592549 w= 0.43471026 b= -0.5302494\n",
      "Epoch: 0110 loss= 0.186323732 w= 0.43448338 b= -0.52861726\n",
      "Epoch: 0111 loss= 0.186055601 w= 0.4342568 b= -0.5269872\n",
      "Epoch: 0112 loss= 0.185788080 w= 0.43403047 b= -0.52535903\n",
      "Epoch: 0113 loss= 0.185521260 w= 0.4338044 b= -0.52373284\n",
      "Epoch: 0114 loss= 0.185255006 w= 0.4335787 b= -0.5221086\n",
      "Epoch: 0115 loss= 0.184989467 w= 0.4333532 b= -0.5204866\n",
      "Epoch: 0116 loss= 0.184724554 w= 0.433128 b= -0.5188664\n",
      "Epoch: 0117 loss= 0.184460357 w= 0.43290305 b= -0.5172483\n",
      "Epoch: 0118 loss= 0.184196725 w= 0.4326783 b= -0.51563203\n",
      "Epoch: 0119 loss= 0.183933809 w= 0.4324539 b= -0.51401794\n",
      "Epoch: 0120 loss= 0.183671504 w= 0.43222982 b= -0.5124057\n",
      "Epoch: 0121 loss= 0.183409825 w= 0.43200597 b= -0.5107955\n",
      "Epoch: 0122 loss= 0.183148831 w= 0.43178245 b= -0.5091872\n",
      "Epoch: 0123 loss= 0.182888433 w= 0.43155915 b= -0.50758094\n",
      "Epoch: 0124 loss= 0.182628691 w= 0.43133613 b= -0.5059766\n",
      "Epoch: 0125 loss= 0.182369605 w= 0.43111342 b= -0.50437427\n",
      "Epoch: 0126 loss= 0.182111144 w= 0.43089092 b= -0.5027738\n",
      "Epoch: 0127 loss= 0.181853250 w= 0.4306688 b= -0.5011754\n",
      "Epoch: 0128 loss= 0.181596071 w= 0.43044686 b= -0.49957895\n",
      "Epoch: 0129 loss= 0.181339487 w= 0.43022522 b= -0.49798438\n",
      "Epoch: 0130 loss= 0.181083545 w= 0.43000382 b= -0.49639177\n",
      "Epoch: 0131 loss= 0.180828214 w= 0.42978272 b= -0.4948011\n",
      "Epoch: 0132 loss= 0.180573508 w= 0.4295619 b= -0.49321246\n",
      "Epoch: 0133 loss= 0.180319428 w= 0.42934132 b= -0.49162573\n",
      "Epoch: 0134 loss= 0.180065975 w= 0.42912102 b= -0.49004084\n",
      "Epoch: 0135 loss= 0.179813161 w= 0.42890102 b= -0.48845798\n",
      "Epoch: 0136 loss= 0.179560959 w= 0.4286812 b= -0.48687705\n",
      "Epoch: 0137 loss= 0.179309383 w= 0.42846173 b= -0.48529813\n",
      "Epoch: 0138 loss= 0.179058358 w= 0.42824253 b= -0.48372105\n",
      "Epoch: 0139 loss= 0.178808019 w= 0.42802355 b= -0.48214597\n",
      "Epoch: 0140 loss= 0.178558305 w= 0.42780492 b= -0.48057273\n",
      "Epoch: 0141 loss= 0.178309128 w= 0.42758647 b= -0.47900146\n",
      "Epoch: 0142 loss= 0.178060621 w= 0.42736837 b= -0.47743213\n",
      "Epoch: 0143 loss= 0.177812710 w= 0.42715046 b= -0.47586474\n",
      "Epoch: 0144 loss= 0.177565396 w= 0.42693278 b= -0.4742992\n",
      "Epoch: 0145 loss= 0.177318707 w= 0.42671546 b= -0.4727356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0146 loss= 0.177072585 w= 0.4264984 b= -0.47117394\n",
      "Epoch: 0147 loss= 0.176827088 w= 0.4262816 b= -0.4696142\n",
      "Epoch: 0148 loss= 0.176582232 w= 0.42606503 b= -0.46805644\n",
      "Epoch: 0149 loss= 0.176337942 w= 0.42584875 b= -0.4665005\n",
      "Epoch: 0150 loss= 0.176094234 w= 0.42563272 b= -0.46494648\n",
      "Epoch: 0151 loss= 0.175851166 w= 0.42541695 b= -0.46339437\n",
      "Epoch: 0152 loss= 0.175608635 w= 0.42520148 b= -0.46184424\n",
      "Epoch: 0153 loss= 0.175366744 w= 0.4249863 b= -0.46029592\n",
      "Epoch: 0154 loss= 0.175125450 w= 0.42477134 b= -0.45874947\n",
      "Epoch: 0155 loss= 0.174884722 w= 0.42455664 b= -0.457205\n",
      "Epoch: 0156 loss= 0.174644619 w= 0.42434216 b= -0.45566243\n",
      "Epoch: 0157 loss= 0.174405068 w= 0.42412803 b= -0.4541217\n",
      "Epoch: 0158 loss= 0.174166113 w= 0.42391407 b= -0.4525829\n",
      "Epoch: 0159 loss= 0.173927739 w= 0.42370042 b= -0.45104593\n",
      "Epoch: 0160 loss= 0.173689976 w= 0.42348704 b= -0.4495109\n",
      "Epoch: 0161 loss= 0.173452765 w= 0.42327395 b= -0.44797772\n",
      "Epoch: 0162 loss= 0.173216134 w= 0.4230611 b= -0.44644642\n",
      "Epoch: 0163 loss= 0.172980130 w= 0.4228485 b= -0.444917\n",
      "Epoch: 0164 loss= 0.172744662 w= 0.42263615 b= -0.44338945\n",
      "Epoch: 0165 loss= 0.172509804 w= 0.42242405 b= -0.44186375\n",
      "Epoch: 0166 loss= 0.172275469 w= 0.42221224 b= -0.44033998\n",
      "Epoch: 0167 loss= 0.172041774 w= 0.42200068 b= -0.43881804\n",
      "Epoch: 0168 loss= 0.171808586 w= 0.42178938 b= -0.43729797\n",
      "Epoch: 0169 loss= 0.171575993 w= 0.42157835 b= -0.43577975\n",
      "Epoch: 0170 loss= 0.171343982 w= 0.42136762 b= -0.4342634\n",
      "Epoch: 0171 loss= 0.171112552 w= 0.42115703 b= -0.4327489\n",
      "Epoch: 0172 loss= 0.170881644 w= 0.4209468 b= -0.4312363\n",
      "Epoch: 0173 loss= 0.170651332 w= 0.42073682 b= -0.42972556\n",
      "Epoch: 0174 loss= 0.170421585 w= 0.420527 b= -0.4282166\n",
      "Epoch: 0175 loss= 0.170192391 w= 0.42031753 b= -0.42670953\n",
      "Epoch: 0176 loss= 0.169963777 w= 0.42010832 b= -0.42520428\n",
      "Epoch: 0177 loss= 0.169735730 w= 0.41989934 b= -0.4237009\n",
      "Epoch: 0178 loss= 0.169508219 w= 0.4196906 b= -0.4221994\n",
      "Epoch: 0179 loss= 0.169281274 w= 0.4194822 b= -0.42069975\n",
      "Epoch: 0180 loss= 0.169054896 w= 0.41927397 b= -0.4192019\n",
      "Epoch: 0181 loss= 0.168829039 w= 0.41906598 b= -0.41770586\n",
      "Epoch: 0182 loss= 0.168603763 w= 0.4188583 b= -0.4162117\n",
      "Epoch: 0183 loss= 0.168379053 w= 0.41865084 b= -0.4147193\n",
      "Epoch: 0184 loss= 0.168154836 w= 0.41844368 b= -0.4132288\n",
      "Epoch: 0185 loss= 0.167931229 w= 0.41823676 b= -0.41174012\n",
      "Epoch: 0186 loss= 0.167708158 w= 0.41803005 b= -0.41025326\n",
      "Epoch: 0187 loss= 0.167485610 w= 0.4178236 b= -0.4087682\n",
      "Epoch: 0188 loss= 0.167263642 w= 0.4176174 b= -0.40728495\n",
      "Epoch: 0189 loss= 0.167042196 w= 0.41741145 b= -0.4058035\n",
      "Epoch: 0190 loss= 0.166821301 w= 0.41720575 b= -0.40432388\n",
      "Epoch: 0191 loss= 0.166600958 w= 0.41700032 b= -0.4028461\n",
      "Epoch: 0192 loss= 0.166381106 w= 0.41679516 b= -0.4013701\n",
      "Epoch: 0193 loss= 0.166161835 w= 0.41659027 b= -0.39989588\n",
      "Epoch: 0194 loss= 0.165943056 w= 0.41638568 b= -0.39842355\n",
      "Epoch: 0195 loss= 0.165724859 w= 0.41618127 b= -0.39695302\n",
      "Epoch: 0196 loss= 0.165507197 w= 0.41597706 b= -0.3954843\n",
      "Epoch: 0197 loss= 0.165290073 w= 0.41577312 b= -0.3940173\n",
      "Epoch: 0198 loss= 0.165073469 w= 0.41556942 b= -0.3925521\n",
      "Epoch: 0199 loss= 0.164857343 w= 0.41536602 b= -0.39108875\n",
      "Epoch: 0200 loss= 0.164641798 w= 0.41516286 b= -0.38962713\n",
      "Epoch: 0201 loss= 0.164426818 w= 0.41495994 b= -0.38816732\n",
      "Epoch: 0202 loss= 0.164212301 w= 0.4147573 b= -0.3867092\n",
      "Epoch: 0203 loss= 0.163998351 w= 0.41455486 b= -0.38525304\n",
      "Epoch: 0204 loss= 0.163784891 w= 0.41435266 b= -0.38379857\n",
      "Epoch: 0205 loss= 0.163571984 w= 0.41415069 b= -0.38234589\n",
      "Epoch: 0206 loss= 0.163359553 w= 0.413949 b= -0.380895\n",
      "Epoch: 0207 loss= 0.163147673 w= 0.41374758 b= -0.37944588\n",
      "Epoch: 0208 loss= 0.162936315 w= 0.41354647 b= -0.37799853\n",
      "Epoch: 0209 loss= 0.162725464 w= 0.4133455 b= -0.37655297\n",
      "Epoch: 0210 loss= 0.162515178 w= 0.41314483 b= -0.3751093\n",
      "Epoch: 0211 loss= 0.162305340 w= 0.4129444 b= -0.3736673\n",
      "Epoch: 0212 loss= 0.162096053 w= 0.41274416 b= -0.37222707\n",
      "Epoch: 0213 loss= 0.161887303 w= 0.41254425 b= -0.37078863\n",
      "Epoch: 0214 loss= 0.161679044 w= 0.41234452 b= -0.36935192\n",
      "Epoch: 0215 loss= 0.161471292 w= 0.412145 b= -0.367917\n",
      "Epoch: 0216 loss= 0.161264032 w= 0.41194576 b= -0.36648384\n",
      "Epoch: 0217 loss= 0.161057338 w= 0.4117468 b= -0.36505237\n",
      "Epoch: 0218 loss= 0.160851076 w= 0.4115481 b= -0.3636227\n",
      "Epoch: 0219 loss= 0.160645351 w= 0.4113496 b= -0.36219487\n",
      "Epoch: 0220 loss= 0.160440132 w= 0.41115132 b= -0.36076865\n",
      "Epoch: 0221 loss= 0.160235405 w= 0.41095334 b= -0.35934427\n",
      "Epoch: 0222 loss= 0.160031199 w= 0.4107556 b= -0.35792163\n",
      "Epoch: 0223 loss= 0.159827530 w= 0.41055804 b= -0.35650074\n",
      "Epoch: 0224 loss= 0.159624293 w= 0.41036084 b= -0.35508165\n",
      "Epoch: 0225 loss= 0.159421548 w= 0.41016382 b= -0.35366422\n",
      "Epoch: 0226 loss= 0.159219354 w= 0.409967 b= -0.35224858\n",
      "Epoch: 0227 loss= 0.159017652 w= 0.4097705 b= -0.3508346\n",
      "Epoch: 0228 loss= 0.158816367 w= 0.40957418 b= -0.34942237\n",
      "Epoch: 0229 loss= 0.158615649 w= 0.40937814 b= -0.34801188\n",
      "Epoch: 0230 loss= 0.158415422 w= 0.4091823 b= -0.34660313\n",
      "Epoch: 0231 loss= 0.158215657 w= 0.4089867 b= -0.34519607\n",
      "Epoch: 0232 loss= 0.158016399 w= 0.4087913 b= -0.34379077\n",
      "Epoch: 0233 loss= 0.157817602 w= 0.40859628 b= -0.34238714\n",
      "Epoch: 0234 loss= 0.157619357 w= 0.4084014 b= -0.3409853\n",
      "Epoch: 0235 loss= 0.157421514 w= 0.40820676 b= -0.33958519\n",
      "Epoch: 0236 loss= 0.157224238 w= 0.4080124 b= -0.33818674\n",
      "Epoch: 0237 loss= 0.157027379 w= 0.40781823 b= -0.33679\n",
      "Epoch: 0238 loss= 0.156831026 w= 0.4076243 b= -0.33539495\n",
      "Epoch: 0239 loss= 0.156635180 w= 0.40743062 b= -0.33400163\n",
      "Epoch: 0240 loss= 0.156439766 w= 0.4072372 b= -0.33261004\n",
      "Epoch: 0241 loss= 0.156244874 w= 0.40704402 b= -0.33122015\n",
      "Epoch: 0242 loss= 0.156050459 w= 0.406851 b= -0.32983202\n",
      "Epoch: 0243 loss= 0.155856505 w= 0.40665823 b= -0.32844555\n",
      "Epoch: 0244 loss= 0.155662999 w= 0.40646574 b= -0.3270607\n",
      "Epoch: 0245 loss= 0.155470014 w= 0.40627345 b= -0.3256776\n",
      "Epoch: 0246 loss= 0.155277476 w= 0.40608147 b= -0.32429624\n",
      "Epoch: 0247 loss= 0.155085400 w= 0.4058897 b= -0.32291654\n",
      "Epoch: 0248 loss= 0.154893845 w= 0.40569815 b= -0.32153857\n",
      "Epoch: 0249 loss= 0.154702723 w= 0.40550682 b= -0.32016224\n",
      "Epoch: 0250 loss= 0.154512063 w= 0.40531573 b= -0.31878763\n",
      "Epoch: 0251 loss= 0.154321864 w= 0.40512487 b= -0.31741467\n",
      "Epoch: 0252 loss= 0.154132187 w= 0.4049343 b= -0.31604344\n",
      "Epoch: 0253 loss= 0.153942898 w= 0.4047439 b= -0.31467378\n",
      "Epoch: 0254 loss= 0.153754130 w= 0.40455377 b= -0.31330588\n",
      "Epoch: 0255 loss= 0.153565794 w= 0.4043638 b= -0.31193963\n",
      "Epoch: 0256 loss= 0.153377920 w= 0.40417415 b= -0.31057507\n",
      "Epoch: 0257 loss= 0.153190508 w= 0.40398473 b= -0.3092121\n",
      "Epoch: 0258 loss= 0.153003559 w= 0.40379548 b= -0.30785084\n",
      "Epoch: 0259 loss= 0.152817070 w= 0.40360644 b= -0.30649126\n",
      "Epoch: 0260 loss= 0.152631029 w= 0.40341774 b= -0.3051333\n",
      "Epoch: 0261 loss= 0.152445450 w= 0.4032292 b= -0.30377716\n",
      "Epoch: 0262 loss= 0.152260378 w= 0.40304095 b= -0.3024226\n",
      "Epoch: 0263 loss= 0.152075663 w= 0.4028529 b= -0.30106968\n",
      "Epoch: 0264 loss= 0.151891455 w= 0.40266508 b= -0.2997184\n",
      "Epoch: 0265 loss= 0.151707694 w= 0.40247744 b= -0.29836884\n",
      "Epoch: 0266 loss= 0.151524395 w= 0.4022901 b= -0.29702094\n",
      "Epoch: 0267 loss= 0.151341543 w= 0.40210292 b= -0.29567462\n",
      "Epoch: 0268 loss= 0.151159152 w= 0.40191594 b= -0.29432997\n",
      "Epoch: 0269 loss= 0.150977194 w= 0.40172926 b= -0.292987\n",
      "Epoch: 0270 loss= 0.150795653 w= 0.4015428 b= -0.29164562\n",
      "Epoch: 0271 loss= 0.150614560 w= 0.4013566 b= -0.29030594\n",
      "Epoch: 0272 loss= 0.150433928 w= 0.4011706 b= -0.2889679\n",
      "Epoch: 0273 loss= 0.150253743 w= 0.40098482 b= -0.28763145\n",
      "Epoch: 0274 loss= 0.150073990 w= 0.40079933 b= -0.2862967\n",
      "Epoch: 0275 loss= 0.149894685 w= 0.400614 b= -0.28496355\n",
      "Epoch: 0276 loss= 0.149715826 w= 0.4004289 b= -0.283632\n",
      "Epoch: 0277 loss= 0.149537414 w= 0.40024403 b= -0.2823021\n",
      "Epoch: 0278 loss= 0.149359390 w= 0.40005934 b= -0.28097385\n",
      "Epoch: 0279 loss= 0.149181843 w= 0.399875 b= -0.27964723\n",
      "Epoch: 0280 loss= 0.149004728 w= 0.3996908 b= -0.27832225\n",
      "Epoch: 0281 loss= 0.148828045 w= 0.3995068 b= -0.27699888\n",
      "Epoch: 0282 loss= 0.148651779 w= 0.39932308 b= -0.27567708\n",
      "Epoch: 0283 loss= 0.148475990 w= 0.39913955 b= -0.27435696\n",
      "Epoch: 0284 loss= 0.148300588 w= 0.39895633 b= -0.27303848\n",
      "Epoch: 0285 loss= 0.148125634 w= 0.3987733 b= -0.27172157\n",
      "Epoch: 0286 loss= 0.147951096 w= 0.39859045 b= -0.2704063\n",
      "Epoch: 0287 loss= 0.147777021 w= 0.39840782 b= -0.26909262\n",
      "Epoch: 0288 loss= 0.147603333 w= 0.39822543 b= -0.2677806\n",
      "Epoch: 0289 loss= 0.147430077 w= 0.3980433 b= -0.26647013\n",
      "Epoch: 0290 loss= 0.147257254 w= 0.39786136 b= -0.26516128\n",
      "Epoch: 0291 loss= 0.147084847 w= 0.39767963 b= -0.263854\n",
      "Epoch: 0292 loss= 0.146912903 w= 0.3974981 b= -0.26254842\n",
      "Epoch: 0293 loss= 0.146741301 w= 0.39731687 b= -0.26124436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0294 loss= 0.146570191 w= 0.39713582 b= -0.2599419\n",
      "Epoch: 0295 loss= 0.146399468 w= 0.396955 b= -0.2586411\n",
      "Epoch: 0296 loss= 0.146229193 w= 0.39677438 b= -0.25734192\n",
      "Epoch: 0297 loss= 0.146059290 w= 0.39659402 b= -0.2560442\n",
      "Epoch: 0298 loss= 0.145889834 w= 0.39641383 b= -0.25474814\n",
      "Epoch: 0299 loss= 0.145720780 w= 0.39623392 b= -0.25345364\n",
      "Epoch: 0300 loss= 0.145552158 w= 0.39605418 b= -0.2521608\n",
      "Epoch: 0301 loss= 0.145383969 w= 0.39587468 b= -0.25086948\n",
      "Epoch: 0302 loss= 0.145216122 w= 0.39569542 b= -0.24957973\n",
      "Epoch: 0303 loss= 0.145048752 w= 0.3955164 b= -0.24829158\n",
      "Epoch: 0304 loss= 0.144881740 w= 0.39533752 b= -0.24700503\n",
      "Epoch: 0305 loss= 0.144715175 w= 0.3951589 b= -0.24572004\n",
      "Epoch: 0306 loss= 0.144548997 w= 0.39498052 b= -0.24443664\n",
      "Epoch: 0307 loss= 0.144383222 w= 0.39480233 b= -0.2431548\n",
      "Epoch: 0308 loss= 0.144217864 w= 0.39462435 b= -0.24187452\n",
      "Epoch: 0309 loss= 0.144052938 w= 0.39444658 b= -0.24059582\n",
      "Epoch: 0310 loss= 0.143888369 w= 0.39426902 b= -0.23931865\n",
      "Epoch: 0311 loss= 0.143724233 w= 0.39409176 b= -0.23804307\n",
      "Epoch: 0312 loss= 0.143560514 w= 0.39391464 b= -0.23676904\n",
      "Epoch: 0313 loss= 0.143397182 w= 0.39373782 b= -0.23549661\n",
      "Epoch: 0314 loss= 0.143234223 w= 0.39356112 b= -0.23422568\n",
      "Epoch: 0315 loss= 0.143071696 w= 0.39338467 b= -0.23295633\n",
      "Epoch: 0316 loss= 0.142909527 w= 0.39320847 b= -0.23168854\n",
      "Epoch: 0317 loss= 0.142747775 w= 0.3930325 b= -0.2304223\n",
      "Epoch: 0318 loss= 0.142586470 w= 0.39285666 b= -0.22915763\n",
      "Epoch: 0319 loss= 0.142425507 w= 0.39268106 b= -0.22789448\n",
      "Epoch: 0320 loss= 0.142264947 w= 0.39250568 b= -0.2266329\n",
      "Epoch: 0321 loss= 0.142104775 w= 0.39233056 b= -0.22537287\n",
      "Epoch: 0322 loss= 0.141945019 w= 0.39215565 b= -0.2241144\n",
      "Epoch: 0323 loss= 0.141785637 w= 0.3919809 b= -0.22285749\n",
      "Epoch: 0324 loss= 0.141626656 w= 0.3918064 b= -0.2216021\n",
      "Epoch: 0325 loss= 0.141468048 w= 0.39163214 b= -0.22034825\n",
      "Epoch: 0326 loss= 0.141309842 w= 0.39145803 b= -0.21909595\n",
      "Epoch: 0327 loss= 0.141152054 w= 0.39128414 b= -0.21784517\n",
      "Epoch: 0328 loss= 0.140994608 w= 0.3911105 b= -0.21659593\n",
      "Epoch: 0329 loss= 0.140837565 w= 0.3909371 b= -0.2153482\n",
      "Epoch: 0330 loss= 0.140680909 w= 0.39076382 b= -0.214102\n",
      "Epoch: 0331 loss= 0.140524626 w= 0.39059082 b= -0.21285735\n",
      "Epoch: 0332 loss= 0.140368745 w= 0.390418 b= -0.21161422\n",
      "Epoch: 0333 loss= 0.140213236 w= 0.39024535 b= -0.2103726\n",
      "Epoch: 0334 loss= 0.140058100 w= 0.390073 b= -0.20913254\n",
      "Epoch: 0335 loss= 0.139903367 w= 0.38990083 b= -0.20789395\n",
      "Epoch: 0336 loss= 0.139748976 w= 0.38972887 b= -0.20665692\n",
      "Epoch: 0337 loss= 0.139594972 w= 0.38955712 b= -0.20542137\n",
      "Epoch: 0338 loss= 0.139441356 w= 0.38938558 b= -0.20418735\n",
      "Epoch: 0339 loss= 0.139288098 w= 0.3892143 b= -0.20295483\n",
      "Epoch: 0340 loss= 0.139135256 w= 0.38904318 b= -0.20172383\n",
      "Epoch: 0341 loss= 0.138982773 w= 0.38887227 b= -0.20049435\n",
      "Epoch: 0342 loss= 0.138830647 w= 0.38870156 b= -0.19926636\n",
      "Epoch: 0343 loss= 0.138678938 w= 0.3885311 b= -0.19803989\n",
      "Epoch: 0344 loss= 0.138527527 w= 0.38836083 b= -0.19681491\n",
      "Epoch: 0345 loss= 0.138376564 w= 0.38819072 b= -0.19559142\n",
      "Epoch: 0346 loss= 0.138225928 w= 0.3880209 b= -0.19436944\n",
      "Epoch: 0347 loss= 0.138075650 w= 0.38785124 b= -0.19314897\n",
      "Epoch: 0348 loss= 0.137925774 w= 0.38768172 b= -0.19192998\n",
      "Epoch: 0349 loss= 0.137776271 w= 0.38751245 b= -0.1907125\n",
      "Epoch: 0350 loss= 0.137627110 w= 0.38734347 b= -0.18949647\n",
      "Epoch: 0351 loss= 0.137478292 w= 0.38717467 b= -0.18828198\n",
      "Epoch: 0352 loss= 0.137329876 w= 0.3870061 b= -0.18706898\n",
      "Epoch: 0353 loss= 0.137181818 w= 0.3868377 b= -0.18585746\n",
      "Epoch: 0354 loss= 0.137034118 w= 0.38666943 b= -0.18464741\n",
      "Epoch: 0355 loss= 0.136886775 w= 0.38650143 b= -0.18343884\n",
      "Epoch: 0356 loss= 0.136739820 w= 0.38633358 b= -0.18223174\n",
      "Epoch: 0357 loss= 0.136593208 w= 0.386166 b= -0.18102615\n",
      "Epoch: 0358 loss= 0.136446938 w= 0.3859987 b= -0.17982203\n",
      "Epoch: 0359 loss= 0.136301041 w= 0.3858315 b= -0.17861938\n",
      "Epoch: 0360 loss= 0.136155501 w= 0.3856645 b= -0.1774182\n",
      "Epoch: 0361 loss= 0.136010319 w= 0.38549775 b= -0.17621848\n",
      "Epoch: 0362 loss= 0.135865524 w= 0.38533115 b= -0.17502028\n",
      "Epoch: 0363 loss= 0.135721028 w= 0.38516483 b= -0.1738235\n",
      "Epoch: 0364 loss= 0.135576889 w= 0.3849987 b= -0.17262821\n",
      "Epoch: 0365 loss= 0.135433152 w= 0.38483277 b= -0.17143437\n",
      "Epoch: 0366 loss= 0.135289744 w= 0.38466704 b= -0.17024203\n",
      "Epoch: 0367 loss= 0.135146677 w= 0.38450146 b= -0.16905114\n",
      "Epoch: 0368 loss= 0.135003999 w= 0.3843361 b= -0.1678617\n",
      "Epoch: 0369 loss= 0.134861603 w= 0.38417098 b= -0.1666737\n",
      "Epoch: 0370 loss= 0.134719595 w= 0.38400605 b= -0.1654872\n",
      "Epoch: 0371 loss= 0.134577930 w= 0.3838414 b= -0.16430211\n",
      "Epoch: 0372 loss= 0.134436652 w= 0.3836768 b= -0.16311848\n",
      "Epoch: 0373 loss= 0.134295627 w= 0.38351247 b= -0.16193631\n",
      "Epoch: 0374 loss= 0.134155050 w= 0.38334835 b= -0.1607556\n",
      "Epoch: 0375 loss= 0.134014755 w= 0.38318443 b= -0.15957631\n",
      "Epoch: 0376 loss= 0.133874819 w= 0.38302073 b= -0.15839846\n",
      "Epoch: 0377 loss= 0.133735240 w= 0.38285717 b= -0.15722206\n",
      "Epoch: 0378 loss= 0.133595988 w= 0.38269386 b= -0.15604714\n",
      "Epoch: 0379 loss= 0.133457080 w= 0.38253072 b= -0.15487362\n",
      "Epoch: 0380 loss= 0.133318529 w= 0.3823678 b= -0.15370157\n",
      "Epoch: 0381 loss= 0.133180276 w= 0.3822051 b= -0.15253094\n",
      "Epoch: 0382 loss= 0.133042410 w= 0.38204256 b= -0.15136172\n",
      "Epoch: 0383 loss= 0.132904842 w= 0.38188028 b= -0.15019397\n",
      "Epoch: 0384 loss= 0.132767633 w= 0.3817181 b= -0.14902763\n",
      "Epoch: 0385 loss= 0.132630765 w= 0.38155612 b= -0.14786272\n",
      "Epoch: 0386 loss= 0.132494226 w= 0.38139442 b= -0.14669923\n",
      "Epoch: 0387 loss= 0.132358029 w= 0.3812329 b= -0.14553718\n",
      "Epoch: 0388 loss= 0.132222131 w= 0.38107154 b= -0.14437655\n",
      "Epoch: 0389 loss= 0.132086620 w= 0.3809104 b= -0.14321736\n",
      "Epoch: 0390 loss= 0.131951407 w= 0.38074946 b= -0.14205956\n",
      "Epoch: 0391 loss= 0.131816521 w= 0.38058877 b= -0.1409032\n",
      "Epoch: 0392 loss= 0.131681994 w= 0.3804282 b= -0.13974825\n",
      "Epoch: 0393 loss= 0.131547809 w= 0.38026783 b= -0.1385947\n",
      "Epoch: 0394 loss= 0.131413862 w= 0.3801077 b= -0.1374426\n",
      "Epoch: 0395 loss= 0.131280348 w= 0.37994772 b= -0.13629192\n",
      "Epoch: 0396 loss= 0.131147102 w= 0.37978795 b= -0.13514262\n",
      "Epoch: 0397 loss= 0.131014198 w= 0.3796284 b= -0.13399471\n",
      "Epoch: 0398 loss= 0.130881637 w= 0.379469 b= -0.13284825\n",
      "Epoch: 0399 loss= 0.130749360 w= 0.3793098 b= -0.13170315\n",
      "Epoch: 0400 loss= 0.130617470 w= 0.37915087 b= -0.13055946\n",
      "Epoch: 0401 loss= 0.130485848 w= 0.37899214 b= -0.1294172\n",
      "Epoch: 0402 loss= 0.130354553 w= 0.37883353 b= -0.12827635\n",
      "Epoch: 0403 loss= 0.130223602 w= 0.37867516 b= -0.12713684\n",
      "Epoch: 0404 loss= 0.130092964 w= 0.37851697 b= -0.12599877\n",
      "Epoch: 0405 loss= 0.129962653 w= 0.37835896 b= -0.12486208\n",
      "Epoch: 0406 loss= 0.129832655 w= 0.3782011 b= -0.12372681\n",
      "Epoch: 0407 loss= 0.129702970 w= 0.37804347 b= -0.1225929\n",
      "Epoch: 0408 loss= 0.129573628 w= 0.3778861 b= -0.1214604\n",
      "Epoch: 0409 loss= 0.129444569 w= 0.3777288 b= -0.12032928\n",
      "Epoch: 0410 loss= 0.129315838 w= 0.37757176 b= -0.11919954\n",
      "Epoch: 0411 loss= 0.129187450 w= 0.3774149 b= -0.11807119\n",
      "Epoch: 0412 loss= 0.129059315 w= 0.3772583 b= -0.116944216\n",
      "Epoch: 0413 loss= 0.128931537 w= 0.37710187 b= -0.11581861\n",
      "Epoch: 0414 loss= 0.128804073 w= 0.37694559 b= -0.114694394\n",
      "Epoch: 0415 loss= 0.128676906 w= 0.37678948 b= -0.11357156\n",
      "Epoch: 0416 loss= 0.128550068 w= 0.37663355 b= -0.1124501\n",
      "Epoch: 0417 loss= 0.128423572 w= 0.37647784 b= -0.111330025\n",
      "Epoch: 0418 loss= 0.128297314 w= 0.37632233 b= -0.11021132\n",
      "Epoch: 0419 loss= 0.128171399 w= 0.376167 b= -0.10909397\n",
      "Epoch: 0420 loss= 0.128045782 w= 0.37601188 b= -0.107978\n",
      "Epoch: 0421 loss= 0.127920479 w= 0.37585697 b= -0.10686342\n",
      "Epoch: 0422 loss= 0.127795517 w= 0.37570217 b= -0.10575018\n",
      "Epoch: 0423 loss= 0.127670825 w= 0.37554765 b= -0.10463831\n",
      "Epoch: 0424 loss= 0.127546445 w= 0.37539336 b= -0.10352779\n",
      "Epoch: 0425 loss= 0.127422363 w= 0.3752392 b= -0.10241865\n",
      "Epoch: 0426 loss= 0.127298579 w= 0.3750852 b= -0.101310864\n",
      "Epoch: 0427 loss= 0.127175108 w= 0.37493137 b= -0.100204445\n",
      "Epoch: 0428 loss= 0.127051964 w= 0.3747777 b= -0.09909938\n",
      "Epoch: 0429 loss= 0.126929104 w= 0.37462425 b= -0.09799567\n",
      "Epoch: 0430 loss= 0.126806542 w= 0.37447104 b= -0.096893325\n",
      "Epoch: 0431 loss= 0.126684278 w= 0.37431797 b= -0.09579232\n",
      "Epoch: 0432 loss= 0.126562327 w= 0.3741651 b= -0.09469265\n",
      "Epoch: 0433 loss= 0.126440659 w= 0.3740124 b= -0.09359433\n",
      "Epoch: 0434 loss= 0.126319304 w= 0.37385994 b= -0.09249735\n",
      "Epoch: 0435 loss= 0.126198202 w= 0.3737077 b= -0.091401696\n",
      "Epoch: 0436 loss= 0.126077473 w= 0.3735556 b= -0.09030743\n",
      "Epoch: 0437 loss= 0.125956982 w= 0.37340367 b= -0.0892145\n",
      "Epoch: 0438 loss= 0.125836805 w= 0.37325197 b= -0.088122874\n",
      "Epoch: 0439 loss= 0.125716910 w= 0.37310043 b= -0.087032616\n",
      "Epoch: 0440 loss= 0.125597343 w= 0.372949 b= -0.08594369\n",
      "Epoch: 0441 loss= 0.125478059 w= 0.3727978 b= -0.08485611\n",
      "Epoch: 0442 loss= 0.125359043 w= 0.3726468 b= -0.083769836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0443 loss= 0.125240341 w= 0.37249595 b= -0.0826849\n",
      "Epoch: 0444 loss= 0.125121921 w= 0.37234533 b= -0.081601284\n",
      "Epoch: 0445 loss= 0.125003785 w= 0.37219492 b= -0.08051902\n",
      "Epoch: 0446 loss= 0.124885939 w= 0.37204468 b= -0.079438075\n",
      "Epoch: 0447 loss= 0.124768384 w= 0.37189463 b= -0.078358434\n",
      "Epoch: 0448 loss= 0.124651127 w= 0.37174475 b= -0.077280134\n",
      "Epoch: 0449 loss= 0.124534130 w= 0.37159503 b= -0.07620315\n",
      "Epoch: 0450 loss= 0.124417476 w= 0.37144548 b= -0.07512749\n",
      "Epoch: 0451 loss= 0.124301068 w= 0.37129614 b= -0.07405312\n",
      "Epoch: 0452 loss= 0.124184944 w= 0.37114698 b= -0.07298009\n",
      "Epoch: 0453 loss= 0.124069087 w= 0.37099802 b= -0.071908385\n",
      "Epoch: 0454 loss= 0.123953551 w= 0.37084922 b= -0.070838\n",
      "Epoch: 0455 loss= 0.123838298 w= 0.37070063 b= -0.06976893\n",
      "Epoch: 0456 loss= 0.123723313 w= 0.3705522 b= -0.06870115\n",
      "Epoch: 0457 loss= 0.123608619 w= 0.37040398 b= -0.06763469\n",
      "Epoch: 0458 loss= 0.123494178 w= 0.3702559 b= -0.06656953\n",
      "Epoch: 0459 loss= 0.123380043 w= 0.37010804 b= -0.06550567\n",
      "Epoch: 0460 loss= 0.123266205 w= 0.36996037 b= -0.06444314\n",
      "Epoch: 0461 loss= 0.123152606 w= 0.36981282 b= -0.06338192\n",
      "Epoch: 0462 loss= 0.123039328 w= 0.36966544 b= -0.06232197\n",
      "Epoch: 0463 loss= 0.122926295 w= 0.36951834 b= -0.06126332\n",
      "Epoch: 0464 loss= 0.122813545 w= 0.3693713 b= -0.060205974\n",
      "Epoch: 0465 loss= 0.122701071 w= 0.3692245 b= -0.059149917\n",
      "Epoch: 0466 loss= 0.122588903 w= 0.36907786 b= -0.05809516\n",
      "Epoch: 0467 loss= 0.122476958 w= 0.36893147 b= -0.057041697\n",
      "Epoch: 0468 loss= 0.122365318 w= 0.36878526 b= -0.055989526\n",
      "Epoch: 0469 loss= 0.122253940 w= 0.3686392 b= -0.054938644\n",
      "Epoch: 0470 loss= 0.122142859 w= 0.36849326 b= -0.053889055\n",
      "Epoch: 0471 loss= 0.122032039 w= 0.3683475 b= -0.052840743\n",
      "Epoch: 0472 loss= 0.121921487 w= 0.368202 b= -0.051793724\n",
      "Epoch: 0473 loss= 0.121811196 w= 0.36805663 b= -0.050747983\n",
      "Epoch: 0474 loss= 0.121701188 w= 0.36791134 b= -0.04970352\n",
      "Epoch: 0475 loss= 0.121591456 w= 0.36776635 b= -0.048660338\n",
      "Epoch: 0476 loss= 0.121481977 w= 0.36762154 b= -0.047618434\n",
      "Epoch: 0477 loss= 0.121372767 w= 0.3674769 b= -0.046577804\n",
      "Epoch: 0478 loss= 0.121263824 w= 0.36733246 b= -0.045538455\n",
      "Epoch: 0479 loss= 0.121155165 w= 0.36718813 b= -0.04450038\n",
      "Epoch: 0480 loss= 0.121046767 w= 0.36704406 b= -0.04346358\n",
      "Epoch: 0481 loss= 0.120938621 w= 0.36690015 b= -0.042428054\n",
      "Epoch: 0482 loss= 0.120830759 w= 0.36675635 b= -0.041393794\n",
      "Epoch: 0483 loss= 0.120723121 w= 0.3666128 b= -0.040360805\n",
      "Epoch: 0484 loss= 0.120615803 w= 0.36646935 b= -0.03932908\n",
      "Epoch: 0485 loss= 0.120508730 w= 0.36632603 b= -0.038298614\n",
      "Epoch: 0486 loss= 0.120401926 w= 0.36618298 b= -0.037269417\n",
      "Epoch: 0487 loss= 0.120295353 w= 0.3660401 b= -0.03624148\n",
      "Epoch: 0488 loss= 0.120189048 w= 0.36589742 b= -0.035214797\n",
      "Epoch: 0489 loss= 0.120083012 w= 0.36575484 b= -0.034189373\n",
      "Epoch: 0490 loss= 0.119977266 w= 0.36561248 b= -0.033165205\n",
      "Epoch: 0491 loss= 0.119871743 w= 0.36547023 b= -0.032142293\n",
      "Epoch: 0492 loss= 0.119766489 w= 0.36532825 b= -0.031120637\n",
      "Epoch: 0493 loss= 0.119661488 w= 0.36518642 b= -0.03010023\n",
      "Epoch: 0494 loss= 0.119556732 w= 0.3650448 b= -0.029081076\n",
      "Epoch: 0495 loss= 0.119452268 w= 0.36490327 b= -0.028063174\n",
      "Epoch: 0496 loss= 0.119348049 w= 0.36476198 b= -0.027046513\n",
      "Epoch: 0497 loss= 0.119244054 w= 0.36462083 b= -0.026031101\n",
      "Epoch: 0498 loss= 0.119140342 w= 0.36447984 b= -0.025016934\n",
      "Epoch: 0499 loss= 0.119036883 w= 0.36433902 b= -0.024004014\n",
      "Epoch: 0500 loss= 0.118933693 w= 0.36419845 b= -0.02299233\n",
      "Epoch: 0501 loss= 0.118830726 w= 0.36405796 b= -0.021981884\n",
      "Epoch: 0502 loss= 0.118728019 w= 0.36391768 b= -0.020972682\n",
      "Epoch: 0503 loss= 0.118625559 w= 0.36377752 b= -0.019964715\n",
      "Epoch: 0504 loss= 0.118523359 w= 0.3636376 b= -0.018957986\n",
      "Epoch: 0505 loss= 0.118421413 w= 0.36349785 b= -0.017952489\n",
      "Epoch: 0506 loss= 0.118319705 w= 0.36335826 b= -0.016948225\n",
      "Epoch: 0507 loss= 0.118218265 w= 0.36321884 b= -0.015945192\n",
      "Epoch: 0508 loss= 0.118117064 w= 0.36307952 b= -0.014943386\n",
      "Epoch: 0509 loss= 0.118016131 w= 0.36294046 b= -0.013942808\n",
      "Epoch: 0510 loss= 0.117915422 w= 0.36280155 b= -0.012943457\n",
      "Epoch: 0511 loss= 0.117814973 w= 0.36266273 b= -0.011945326\n",
      "Epoch: 0512 loss= 0.117714725 w= 0.36252418 b= -0.010948417\n",
      "Epoch: 0513 loss= 0.117614776 w= 0.36238578 b= -0.009952733\n",
      "Epoch: 0514 loss= 0.117515042 w= 0.3622476 b= -0.008958271\n",
      "Epoch: 0515 loss= 0.117415577 w= 0.36210954 b= -0.00796503\n",
      "Epoch: 0516 loss= 0.117316335 w= 0.36197162 b= -0.006973006\n",
      "Epoch: 0517 loss= 0.117217340 w= 0.3618339 b= -0.0059821964\n",
      "Epoch: 0518 loss= 0.117118612 w= 0.36169636 b= -0.0049926024\n",
      "Epoch: 0519 loss= 0.117020093 w= 0.36155897 b= -0.0040042214\n",
      "Epoch: 0520 loss= 0.116921835 w= 0.36142173 b= -0.0030170504\n",
      "Epoch: 0521 loss= 0.116823807 w= 0.36128464 b= -0.0020310883\n",
      "Epoch: 0522 loss= 0.116726026 w= 0.36114776 b= -0.0010463328\n",
      "Epoch: 0523 loss= 0.116628498 w= 0.36101103 b= -6.278606e-05\n",
      "Epoch: 0524 loss= 0.116531193 w= 0.36087453 b= 0.0009195546\n",
      "Epoch: 0525 loss= 0.116434112 w= 0.36073813 b= 0.0019006904\n",
      "Epoch: 0526 loss= 0.116337307 w= 0.3606019 b= 0.0028806238\n",
      "Epoch: 0527 loss= 0.116240695 w= 0.36046585 b= 0.0038593574\n",
      "Epoch: 0528 loss= 0.116144367 w= 0.36032996 b= 0.0048368922\n",
      "Epoch: 0529 loss= 0.116048239 w= 0.36019418 b= 0.0058132303\n",
      "Epoch: 0530 loss= 0.115952365 w= 0.36005867 b= 0.0067883725\n",
      "Epoch: 0531 loss= 0.115856729 w= 0.35992327 b= 0.0077623203\n",
      "Epoch: 0532 loss= 0.115761317 w= 0.35978806 b= 0.008735076\n",
      "Epoch: 0533 loss= 0.115666136 w= 0.35965303 b= 0.009706632\n",
      "Epoch: 0534 loss= 0.115571208 w= 0.3595181 b= 0.010677004\n",
      "Epoch: 0535 loss= 0.115476482 w= 0.35938337 b= 0.011646186\n",
      "Epoch: 0536 loss= 0.115382016 w= 0.35924882 b= 0.012614181\n",
      "Epoch: 0537 loss= 0.115287751 w= 0.3591144 b= 0.013580987\n",
      "Epoch: 0538 loss= 0.115193740 w= 0.35898018 b= 0.014546611\n",
      "Epoch: 0539 loss= 0.115099967 w= 0.3588461 b= 0.015511051\n",
      "Epoch: 0540 loss= 0.115006395 w= 0.3587123 b= 0.016474312\n",
      "Epoch: 0541 loss= 0.114913076 w= 0.35857853 b= 0.017436387\n",
      "Epoch: 0542 loss= 0.114819981 w= 0.358445 b= 0.018397283\n",
      "Epoch: 0543 loss= 0.114727110 w= 0.35831162 b= 0.019356996\n",
      "Epoch: 0544 loss= 0.114634469 w= 0.35817835 b= 0.020315532\n",
      "Epoch: 0545 loss= 0.114542045 w= 0.35804528 b= 0.021272898\n",
      "Epoch: 0546 loss= 0.114449874 w= 0.35791236 b= 0.022229088\n",
      "Epoch: 0547 loss= 0.114357896 w= 0.35777965 b= 0.023184106\n",
      "Epoch: 0548 loss= 0.114266180 w= 0.35764703 b= 0.024137955\n",
      "Epoch: 0549 loss= 0.114174664 w= 0.3575146 b= 0.025090635\n",
      "Epoch: 0550 loss= 0.114083387 w= 0.35738233 b= 0.026042143\n",
      "Epoch: 0551 loss= 0.113992326 w= 0.35725024 b= 0.026992487\n",
      "Epoch: 0552 loss= 0.113901459 w= 0.35711834 b= 0.027941667\n",
      "Epoch: 0553 loss= 0.113810867 w= 0.35698655 b= 0.028889678\n",
      "Epoch: 0554 loss= 0.113720469 w= 0.35685486 b= 0.029836535\n",
      "Epoch: 0555 loss= 0.113630317 w= 0.35672337 b= 0.030782234\n",
      "Epoch: 0556 loss= 0.113540359 w= 0.35659206 b= 0.03172677\n",
      "Epoch: 0557 loss= 0.113450639 w= 0.35646093 b= 0.032670163\n",
      "Epoch: 0558 loss= 0.113361105 w= 0.35633 b= 0.033612393\n",
      "Epoch: 0559 loss= 0.113271840 w= 0.35619918 b= 0.034553457\n",
      "Epoch: 0560 loss= 0.113182746 w= 0.3560685 b= 0.035493366\n",
      "Epoch: 0561 loss= 0.113093905 w= 0.35593802 b= 0.036432132\n",
      "Epoch: 0562 loss= 0.113005266 w= 0.35580763 b= 0.03736975\n",
      "Epoch: 0563 loss= 0.112916842 w= 0.3556775 b= 0.038306218\n",
      "Epoch: 0564 loss= 0.112828642 w= 0.3555475 b= 0.039241537\n",
      "Epoch: 0565 loss= 0.112740651 w= 0.35541767 b= 0.04017571\n",
      "Epoch: 0566 loss= 0.112652883 w= 0.35528794 b= 0.04110874\n",
      "Epoch: 0567 loss= 0.112565346 w= 0.35515845 b= 0.042040624\n",
      "Epoch: 0568 loss= 0.112477995 w= 0.35502914 b= 0.042971358\n",
      "Epoch: 0569 loss= 0.112390861 w= 0.35489988 b= 0.04390095\n",
      "Epoch: 0570 loss= 0.112303957 w= 0.35477084 b= 0.044829406\n",
      "Epoch: 0571 loss= 0.112217255 w= 0.3546419 b= 0.04575672\n",
      "Epoch: 0572 loss= 0.112130769 w= 0.35451314 b= 0.0466829\n",
      "Epoch: 0573 loss= 0.112044521 w= 0.3543845 b= 0.04760795\n",
      "Epoch: 0574 loss= 0.111958452 w= 0.3542561 b= 0.048531868\n",
      "Epoch: 0575 loss= 0.111872606 w= 0.35412776 b= 0.04945464\n",
      "Epoch: 0576 loss= 0.111786962 w= 0.3539997 b= 0.050376292\n",
      "Epoch: 0577 loss= 0.111701533 w= 0.3538718 b= 0.05129681\n",
      "Epoch: 0578 loss= 0.111616313 w= 0.353744 b= 0.05221621\n",
      "Epoch: 0579 loss= 0.111531302 w= 0.35361636 b= 0.053134464\n",
      "Epoch: 0580 loss= 0.111446492 w= 0.35348883 b= 0.054051597\n",
      "Epoch: 0581 loss= 0.111361913 w= 0.3533615 b= 0.05496762\n",
      "Epoch: 0582 loss= 0.111277513 w= 0.35323432 b= 0.05588252\n",
      "Epoch: 0583 loss= 0.111193322 w= 0.35310733 b= 0.0567963\n",
      "Epoch: 0584 loss= 0.111109354 w= 0.35298046 b= 0.057708956\n",
      "Epoch: 0585 loss= 0.111025594 w= 0.35285378 b= 0.058620486\n",
      "Epoch: 0586 loss= 0.110942028 w= 0.35272723 b= 0.0595309\n",
      "Epoch: 0587 loss= 0.110858679 w= 0.3526008 b= 0.0604402\n",
      "Epoch: 0588 loss= 0.110775515 w= 0.35247454 b= 0.061348382\n",
      "Epoch: 0589 loss= 0.110692568 w= 0.35234845 b= 0.06225547\n",
      "Epoch: 0590 loss= 0.110609829 w= 0.3522225 b= 0.06316145\n",
      "Epoch: 0591 loss= 0.110527292 w= 0.35209668 b= 0.06406631\n",
      "Epoch: 0592 loss= 0.110444948 w= 0.3519711 b= 0.06497007\n",
      "Epoch: 0593 loss= 0.110362813 w= 0.35184562 b= 0.0658727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0594 loss= 0.110280856 w= 0.35172027 b= 0.06677423\n",
      "Epoch: 0595 loss= 0.110199139 w= 0.3515951 b= 0.06767466\n",
      "Epoch: 0596 loss= 0.110117599 w= 0.35147014 b= 0.06857399\n",
      "Epoch: 0597 loss= 0.110036261 w= 0.3513453 b= 0.069472216\n",
      "Epoch: 0598 loss= 0.109955117 w= 0.35122058 b= 0.070369326\n",
      "Epoch: 0599 loss= 0.109874189 w= 0.35109597 b= 0.07126535\n",
      "Epoch: 0600 loss= 0.109793462 w= 0.35097155 b= 0.07216028\n",
      "Epoch: 0601 loss= 0.109712906 w= 0.35084733 b= 0.0730541\n",
      "Epoch: 0602 loss= 0.109632559 w= 0.3507232 b= 0.07394684\n",
      "Epoch: 0603 loss= 0.109552406 w= 0.35059932 b= 0.07483848\n",
      "Epoch: 0604 loss= 0.109472468 w= 0.35047552 b= 0.07572903\n",
      "Epoch: 0605 loss= 0.109392710 w= 0.35035184 b= 0.07661849\n",
      "Epoch: 0606 loss= 0.109313153 w= 0.35022837 b= 0.07750685\n",
      "Epoch: 0607 loss= 0.109233789 w= 0.35010505 b= 0.078394115\n",
      "Epoch: 0608 loss= 0.109154619 w= 0.34998187 b= 0.0792803\n",
      "Epoch: 0609 loss= 0.109075636 w= 0.34985888 b= 0.08016539\n",
      "Epoch: 0610 loss= 0.108996883 w= 0.34973598 b= 0.081049405\n",
      "Epoch: 0611 loss= 0.108918272 w= 0.34961328 b= 0.08193233\n",
      "Epoch: 0612 loss= 0.108839892 w= 0.34949067 b= 0.08281419\n",
      "Epoch: 0613 loss= 0.108761683 w= 0.3493682 b= 0.08369496\n",
      "Epoch: 0614 loss= 0.108683676 w= 0.3492459 b= 0.08457464\n",
      "Epoch: 0615 loss= 0.108605847 w= 0.34912378 b= 0.08545326\n",
      "Epoch: 0616 loss= 0.108528234 w= 0.34900177 b= 0.08633078\n",
      "Epoch: 0617 loss= 0.108450815 w= 0.34887996 b= 0.08720724\n",
      "Epoch: 0618 loss= 0.108373545 w= 0.34875825 b= 0.08808262\n",
      "Epoch: 0619 loss= 0.108296491 w= 0.34863672 b= 0.08895694\n",
      "Epoch: 0620 loss= 0.108219646 w= 0.34851527 b= 0.089830175\n",
      "Epoch: 0621 loss= 0.108142942 w= 0.34839407 b= 0.090702355\n",
      "Epoch: 0622 loss= 0.108066432 w= 0.34827304 b= 0.091573454\n",
      "Epoch: 0623 loss= 0.107990131 w= 0.34815213 b= 0.092443496\n",
      "Epoch: 0624 loss= 0.107914008 w= 0.3480313 b= 0.09331247\n",
      "Epoch: 0625 loss= 0.107838079 w= 0.3479107 b= 0.09418037\n",
      "Epoch: 0626 loss= 0.107762337 w= 0.34779018 b= 0.0950472\n",
      "Epoch: 0627 loss= 0.107686773 w= 0.3476698 b= 0.09591295\n",
      "Epoch: 0628 loss= 0.107611395 w= 0.34754962 b= 0.096777655\n",
      "Epoch: 0629 loss= 0.107536227 w= 0.3474296 b= 0.09764132\n",
      "Epoch: 0630 loss= 0.107461214 w= 0.34730968 b= 0.0985039\n",
      "Epoch: 0631 loss= 0.107386395 w= 0.34718993 b= 0.09936544\n",
      "Epoch: 0632 loss= 0.107311755 w= 0.34707028 b= 0.10022592\n",
      "Epoch: 0633 loss= 0.107237302 w= 0.34695083 b= 0.10108534\n",
      "Epoch: 0634 loss= 0.107163027 w= 0.34683153 b= 0.10194372\n",
      "Epoch: 0635 loss= 0.107088931 w= 0.34671238 b= 0.102801055\n",
      "Epoch: 0636 loss= 0.107015029 w= 0.34659335 b= 0.103657335\n",
      "Epoch: 0637 loss= 0.106941290 w= 0.3464745 b= 0.10451256\n",
      "Epoch: 0638 loss= 0.106867753 w= 0.34635577 b= 0.105366744\n",
      "Epoch: 0639 loss= 0.106794395 w= 0.34623715 b= 0.106219865\n",
      "Epoch: 0640 loss= 0.106721222 w= 0.3461187 b= 0.107071966\n",
      "Epoch: 0641 loss= 0.106648199 w= 0.34600034 b= 0.10792301\n",
      "Epoch: 0642 loss= 0.106575377 w= 0.34588224 b= 0.108773015\n",
      "Epoch: 0643 loss= 0.106502719 w= 0.34576422 b= 0.10962194\n",
      "Epoch: 0644 loss= 0.106430270 w= 0.34564632 b= 0.11046985\n",
      "Epoch: 0645 loss= 0.106357977 w= 0.34552863 b= 0.11131672\n",
      "Epoch: 0646 loss= 0.106285825 w= 0.3454111 b= 0.11216254\n",
      "Epoch: 0647 loss= 0.106213912 w= 0.3452936 b= 0.113007344\n",
      "Epoch: 0648 loss= 0.106142141 w= 0.34517634 b= 0.11385111\n",
      "Epoch: 0649 loss= 0.106070578 w= 0.34505913 b= 0.11469384\n",
      "Epoch: 0650 loss= 0.105999164 w= 0.34494215 b= 0.11553555\n",
      "Epoch: 0651 loss= 0.105927937 w= 0.34482527 b= 0.11637622\n",
      "Epoch: 0652 loss= 0.105856858 w= 0.3447086 b= 0.11721585\n",
      "Epoch: 0653 loss= 0.105785981 w= 0.344592 b= 0.11805445\n",
      "Epoch: 0654 loss= 0.105715267 w= 0.34447563 b= 0.118892014\n",
      "Epoch: 0655 loss= 0.105644725 w= 0.34435934 b= 0.11972857\n",
      "Epoch: 0656 loss= 0.105574362 w= 0.34424323 b= 0.1205641\n",
      "Epoch: 0657 loss= 0.105504185 w= 0.34412718 b= 0.12139861\n",
      "Epoch: 0658 loss= 0.105434135 w= 0.3440113 b= 0.12223209\n",
      "Epoch: 0659 loss= 0.105364285 w= 0.34389564 b= 0.123064555\n",
      "Epoch: 0660 loss= 0.105294608 w= 0.34378007 b= 0.123895995\n",
      "Epoch: 0661 loss= 0.105225101 w= 0.34366465 b= 0.12472641\n",
      "Epoch: 0662 loss= 0.105155773 w= 0.3435493 b= 0.12555578\n",
      "Epoch: 0663 loss= 0.105086625 w= 0.34343415 b= 0.12638415\n",
      "Epoch: 0664 loss= 0.105017610 w= 0.34331912 b= 0.12721154\n",
      "Epoch: 0665 loss= 0.104948796 w= 0.3432043 b= 0.12803786\n",
      "Epoch: 0666 loss= 0.104880124 w= 0.34308955 b= 0.1288632\n",
      "Epoch: 0667 loss= 0.104811646 w= 0.34297496 b= 0.12968755\n",
      "Epoch: 0668 loss= 0.104743317 w= 0.3428605 b= 0.13051085\n",
      "Epoch: 0669 loss= 0.104675174 w= 0.34274614 b= 0.13133314\n",
      "Epoch: 0670 loss= 0.104607180 w= 0.34263203 b= 0.13215443\n",
      "Epoch: 0671 loss= 0.104539365 w= 0.342518 b= 0.13297476\n",
      "Epoch: 0672 loss= 0.104471698 w= 0.34240416 b= 0.13379404\n",
      "Epoch: 0673 loss= 0.104404218 w= 0.3422904 b= 0.13461232\n",
      "Epoch: 0674 loss= 0.104336888 w= 0.34217685 b= 0.1354296\n",
      "Epoch: 0675 loss= 0.104269736 w= 0.34206334 b= 0.13624586\n",
      "Epoch: 0676 loss= 0.104202740 w= 0.34195 b= 0.13706118\n",
      "Epoch: 0677 loss= 0.104135916 w= 0.34183678 b= 0.13787547\n",
      "Epoch: 0678 loss= 0.104069248 w= 0.3417237 b= 0.13868876\n",
      "Epoch: 0679 loss= 0.104002737 w= 0.3416108 b= 0.13950105\n",
      "Epoch: 0680 loss= 0.103936397 w= 0.34149805 b= 0.14031236\n",
      "Epoch: 0681 loss= 0.103870220 w= 0.34138542 b= 0.14112267\n",
      "Epoch: 0682 loss= 0.103804208 w= 0.34127295 b= 0.14193198\n",
      "Epoch: 0683 loss= 0.103738375 w= 0.3411605 b= 0.1427403\n",
      "Epoch: 0684 loss= 0.103672676 w= 0.3410483 b= 0.14354764\n",
      "Epoch: 0685 loss= 0.103607126 w= 0.34093624 b= 0.14435399\n",
      "Epoch: 0686 loss= 0.103541762 w= 0.34082425 b= 0.14515932\n",
      "Epoch: 0687 loss= 0.103476577 w= 0.34071243 b= 0.14596365\n",
      "Epoch: 0688 loss= 0.103411518 w= 0.34060076 b= 0.14676704\n",
      "Epoch: 0689 loss= 0.103346638 w= 0.34048924 b= 0.14756943\n",
      "Epoch: 0690 loss= 0.103281893 w= 0.34037784 b= 0.14837085\n",
      "Epoch: 0691 loss= 0.103217311 w= 0.34026662 b= 0.14917131\n",
      "Epoch: 0692 loss= 0.103152908 w= 0.34015548 b= 0.14997071\n",
      "Epoch: 0693 loss= 0.103088677 w= 0.34004444 b= 0.15076919\n",
      "Epoch: 0694 loss= 0.103024557 w= 0.3399336 b= 0.1515667\n",
      "Epoch: 0695 loss= 0.102960631 w= 0.33982292 b= 0.1523632\n",
      "Epoch: 0696 loss= 0.102896832 w= 0.33971238 b= 0.15315871\n",
      "Epoch: 0697 loss= 0.102833219 w= 0.33960196 b= 0.15395324\n",
      "Epoch: 0698 loss= 0.102769732 w= 0.33949167 b= 0.1547468\n",
      "Epoch: 0699 loss= 0.102706425 w= 0.3393815 b= 0.15553941\n",
      "Epoch: 0700 loss= 0.102643274 w= 0.3392714 b= 0.15633105\n",
      "Epoch: 0701 loss= 0.102580272 w= 0.33916146 b= 0.15712172\n",
      "Epoch: 0702 loss= 0.102517426 w= 0.33905166 b= 0.15791142\n",
      "Epoch: 0703 loss= 0.102454729 w= 0.33894205 b= 0.15870015\n",
      "Epoch: 0704 loss= 0.102392182 w= 0.33883256 b= 0.15948789\n",
      "Epoch: 0705 loss= 0.102329798 w= 0.3387232 b= 0.1602747\n",
      "Epoch: 0706 loss= 0.102267578 w= 0.33861393 b= 0.16106053\n",
      "Epoch: 0707 loss= 0.102205493 w= 0.33850482 b= 0.16184539\n",
      "Epoch: 0708 loss= 0.102143563 w= 0.33839586 b= 0.16262929\n",
      "Epoch: 0709 loss= 0.102081776 w= 0.33828706 b= 0.16341226\n",
      "Epoch: 0710 loss= 0.102020137 w= 0.33817837 b= 0.16419424\n",
      "Epoch: 0711 loss= 0.101958677 w= 0.3380698 b= 0.16497526\n",
      "Epoch: 0712 loss= 0.101897344 w= 0.33796138 b= 0.16575535\n",
      "Epoch: 0713 loss= 0.101836160 w= 0.33785304 b= 0.16653445\n",
      "Epoch: 0714 loss= 0.101775147 w= 0.3377449 b= 0.16731258\n",
      "Epoch: 0715 loss= 0.101714283 w= 0.33763686 b= 0.16808978\n",
      "Epoch: 0716 loss= 0.101653554 w= 0.33752888 b= 0.168866\n",
      "Epoch: 0717 loss= 0.101592995 w= 0.3374211 b= 0.1696413\n",
      "Epoch: 0718 loss= 0.101532564 w= 0.33731344 b= 0.17041564\n",
      "Epoch: 0719 loss= 0.101472266 w= 0.33720598 b= 0.17118905\n",
      "Epoch: 0720 loss= 0.101412147 w= 0.3370986 b= 0.1719615\n",
      "Epoch: 0721 loss= 0.101352200 w= 0.3369913 b= 0.172733\n",
      "Epoch: 0722 loss= 0.101292312 w= 0.33688423 b= 0.1735036\n",
      "Epoch: 0723 loss= 0.101232626 w= 0.33677724 b= 0.17427321\n",
      "Epoch: 0724 loss= 0.101173081 w= 0.3366704 b= 0.17504191\n",
      "Epoch: 0725 loss= 0.101113677 w= 0.33656377 b= 0.17580965\n",
      "Epoch: 0726 loss= 0.101054430 w= 0.3364572 b= 0.17657645\n",
      "Epoch: 0727 loss= 0.100995347 w= 0.3363507 b= 0.17734231\n",
      "Epoch: 0728 loss= 0.100936376 w= 0.33624434 b= 0.17810725\n",
      "Epoch: 0729 loss= 0.100877553 w= 0.3361381 b= 0.17887121\n",
      "Epoch: 0730 loss= 0.100818887 w= 0.33603203 b= 0.17963426\n",
      "Epoch: 0731 loss= 0.100760356 w= 0.33592615 b= 0.18039636\n",
      "Epoch: 0732 loss= 0.100701973 w= 0.3358203 b= 0.18115753\n",
      "Epoch: 0733 loss= 0.100643717 w= 0.33571464 b= 0.18191776\n",
      "Epoch: 0734 loss= 0.100585625 w= 0.3356091 b= 0.18267706\n",
      "Epoch: 0735 loss= 0.100527667 w= 0.33550367 b= 0.18343547\n",
      "Epoch: 0736 loss= 0.100469857 w= 0.33539835 b= 0.18419293\n",
      "Epoch: 0737 loss= 0.100412197 w= 0.33529314 b= 0.18494944\n",
      "Epoch: 0738 loss= 0.100354642 w= 0.33518815 b= 0.18570505\n",
      "Epoch: 0739 loss= 0.100297257 w= 0.33508328 b= 0.18645974\n",
      "Epoch: 0740 loss= 0.100240000 w= 0.3349785 b= 0.1872135\n",
      "Epoch: 0741 loss= 0.100182891 w= 0.33487386 b= 0.18796635\n",
      "Epoch: 0742 loss= 0.100125924 w= 0.33476934 b= 0.18871824\n",
      "Epoch: 0743 loss= 0.100069113 w= 0.33466497 b= 0.18946922\n",
      "Epoch: 0744 loss= 0.100012407 w= 0.33456072 b= 0.1902193\n",
      "Epoch: 0745 loss= 0.099955849 w= 0.33445656 b= 0.19096845\n",
      "Epoch: 0746 loss= 0.099899448 w= 0.33435252 b= 0.19171666\n",
      "Epoch: 0747 loss= 0.099843152 w= 0.3342487 b= 0.19246398\n",
      "Epoch: 0748 loss= 0.099787027 w= 0.33414492 b= 0.19321035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0749 loss= 0.099731028 w= 0.3340413 b= 0.19395582\n",
      "Epoch: 0750 loss= 0.099675149 w= 0.33393782 b= 0.19470039\n",
      "Epoch: 0751 loss= 0.099619448 w= 0.33383444 b= 0.19544403\n",
      "Epoch: 0752 loss= 0.099563874 w= 0.33373117 b= 0.19618678\n",
      "Epoch: 0753 loss= 0.099508412 w= 0.333628 b= 0.1969286\n",
      "Epoch: 0754 loss= 0.099453107 w= 0.333525 b= 0.19766952\n",
      "Epoch: 0755 loss= 0.099397928 w= 0.33342218 b= 0.19840951\n",
      "Epoch: 0756 loss= 0.099342875 w= 0.3333194 b= 0.19914863\n",
      "Epoch: 0757 loss= 0.099287987 w= 0.33321676 b= 0.19988684\n",
      "Epoch: 0758 loss= 0.099233195 w= 0.3331143 b= 0.20062411\n",
      "Epoch: 0759 loss= 0.099178568 w= 0.33301193 b= 0.2013605\n",
      "Epoch: 0760 loss= 0.099124067 w= 0.3329097 b= 0.20209599\n",
      "Epoch: 0761 loss= 0.099069692 w= 0.3328076 b= 0.2028306\n",
      "Epoch: 0762 loss= 0.099015430 w= 0.33270562 b= 0.20356429\n",
      "Epoch: 0763 loss= 0.098961338 w= 0.33260372 b= 0.20429705\n",
      "Epoch: 0764 loss= 0.098907366 w= 0.33250204 b= 0.20502894\n",
      "Epoch: 0765 loss= 0.098853521 w= 0.3324004 b= 0.20575991\n",
      "Epoch: 0766 loss= 0.098799787 w= 0.33229896 b= 0.20648998\n",
      "Epoch: 0767 loss= 0.098746248 w= 0.3321976 b= 0.20721917\n",
      "Epoch: 0768 loss= 0.098692782 w= 0.3320964 b= 0.20794746\n",
      "Epoch: 0769 loss= 0.098639496 w= 0.33199525 b= 0.20867486\n",
      "Epoch: 0770 loss= 0.098586299 w= 0.3318943 b= 0.20940137\n",
      "Epoch: 0771 loss= 0.098533250 w= 0.33179343 b= 0.210127\n",
      "Epoch: 0772 loss= 0.098480314 w= 0.33169267 b= 0.21085174\n",
      "Epoch: 0773 loss= 0.098427519 w= 0.33159205 b= 0.21157561\n",
      "Epoch: 0774 loss= 0.098374873 w= 0.33149153 b= 0.21229857\n",
      "Epoch: 0775 loss= 0.098322332 w= 0.3313912 b= 0.21302064\n",
      "Epoch: 0776 loss= 0.098269925 w= 0.33129093 b= 0.21374184\n",
      "Epoch: 0777 loss= 0.098217659 w= 0.3311908 b= 0.21446218\n",
      "Epoch: 0778 loss= 0.098165475 w= 0.33109084 b= 0.21518162\n",
      "Epoch: 0779 loss= 0.098113477 w= 0.33099097 b= 0.21590017\n",
      "Epoch: 0780 loss= 0.098061599 w= 0.33089116 b= 0.21661782\n",
      "Epoch: 0781 loss= 0.098009817 w= 0.33079153 b= 0.21733461\n",
      "Epoch: 0782 loss= 0.097958162 w= 0.33069202 b= 0.21805048\n",
      "Epoch: 0783 loss= 0.097906679 w= 0.3305926 b= 0.2187655\n",
      "Epoch: 0784 loss= 0.097855277 w= 0.3304933 b= 0.21947964\n",
      "Epoch: 0785 loss= 0.097804032 w= 0.33039415 b= 0.2201929\n",
      "Epoch: 0786 loss= 0.097752906 w= 0.33029512 b= 0.22090527\n",
      "Epoch: 0787 loss= 0.097701885 w= 0.3301962 b= 0.2216168\n",
      "Epoch: 0788 loss= 0.097651005 w= 0.33009744 b= 0.2223275\n",
      "Epoch: 0789 loss= 0.097600251 w= 0.32999882 b= 0.22303727\n",
      "Epoch: 0790 loss= 0.097549617 w= 0.32990023 b= 0.2237462\n",
      "Epoch: 0791 loss= 0.097499132 w= 0.32980177 b= 0.22445421\n",
      "Epoch: 0792 loss= 0.097448736 w= 0.32970345 b= 0.22516142\n",
      "Epoch: 0793 loss= 0.097398467 w= 0.3296053 b= 0.22586775\n",
      "Epoch: 0794 loss= 0.097348318 w= 0.32950726 b= 0.22657317\n",
      "Epoch: 0795 loss= 0.097298309 w= 0.3294093 b= 0.22727779\n",
      "Epoch: 0796 loss= 0.097248413 w= 0.32931155 b= 0.22798151\n",
      "Epoch: 0797 loss= 0.097198643 w= 0.32921386 b= 0.22868437\n",
      "Epoch: 0798 loss= 0.097148985 w= 0.32911623 b= 0.22938637\n",
      "Epoch: 0799 loss= 0.097099483 w= 0.32901874 b= 0.23008752\n",
      "Epoch: 0800 loss= 0.097050063 w= 0.32892138 b= 0.23078781\n",
      "Epoch: 0801 loss= 0.097000785 w= 0.32882416 b= 0.23148726\n",
      "Epoch: 0802 loss= 0.096951626 w= 0.32872704 b= 0.23218584\n",
      "Epoch: 0803 loss= 0.096902557 w= 0.32863012 b= 0.23288354\n",
      "Epoch: 0804 loss= 0.096853666 w= 0.3285332 b= 0.23358038\n",
      "Epoch: 0805 loss= 0.096804857 w= 0.32843643 b= 0.23427637\n",
      "Epoch: 0806 loss= 0.096756168 w= 0.32833984 b= 0.23497151\n",
      "Epoch: 0807 loss= 0.096707605 w= 0.32824332 b= 0.23566584\n",
      "Epoch: 0808 loss= 0.096659154 w= 0.3281469 b= 0.2363593\n",
      "Epoch: 0809 loss= 0.096610822 w= 0.32805067 b= 0.23705192\n",
      "Epoch: 0810 loss= 0.096562617 w= 0.32795453 b= 0.23774365\n",
      "Epoch: 0811 loss= 0.096514530 w= 0.3278585 b= 0.23843457\n",
      "Epoch: 0812 loss= 0.096466564 w= 0.32776254 b= 0.23912457\n",
      "Epoch: 0813 loss= 0.096418716 w= 0.32766676 b= 0.23981376\n",
      "Epoch: 0814 loss= 0.096370995 w= 0.3275711 b= 0.24050212\n",
      "Epoch: 0815 loss= 0.096323356 w= 0.32747555 b= 0.24118963\n",
      "Epoch: 0816 loss= 0.096275851 w= 0.3273801 b= 0.24187632\n",
      "Epoch: 0817 loss= 0.096228480 w= 0.3272847 b= 0.24256216\n",
      "Epoch: 0818 loss= 0.096181199 w= 0.3271895 b= 0.24324715\n",
      "Epoch: 0819 loss= 0.096134059 w= 0.32709438 b= 0.24393126\n",
      "Epoch: 0820 loss= 0.096087016 w= 0.32699946 b= 0.24461457\n",
      "Epoch: 0821 loss= 0.096040092 w= 0.32690457 b= 0.24529709\n",
      "Epoch: 0822 loss= 0.095993280 w= 0.32680976 b= 0.24597873\n",
      "Epoch: 0823 loss= 0.095946610 w= 0.3267151 b= 0.24665953\n",
      "Epoch: 0824 loss= 0.095900021 w= 0.32662055 b= 0.24733952\n",
      "Epoch: 0825 loss= 0.095853560 w= 0.32652614 b= 0.24801867\n",
      "Epoch: 0826 loss= 0.095807195 w= 0.32643187 b= 0.24869698\n",
      "Epoch: 0827 loss= 0.095760979 w= 0.32633778 b= 0.24937443\n",
      "Epoch: 0828 loss= 0.095714837 w= 0.32624373 b= 0.2500511\n",
      "Epoch: 0829 loss= 0.095668830 w= 0.3261498 b= 0.2507269\n",
      "Epoch: 0830 loss= 0.095622964 w= 0.32605594 b= 0.2514018\n",
      "Epoch: 0831 loss= 0.095577165 w= 0.32596222 b= 0.25207597\n",
      "Epoch: 0832 loss= 0.095531508 w= 0.32586867 b= 0.25274926\n",
      "Epoch: 0833 loss= 0.095485941 w= 0.3257752 b= 0.25342178\n",
      "Epoch: 0834 loss= 0.095440485 w= 0.32568187 b= 0.25409347\n",
      "Epoch: 0835 loss= 0.095395155 w= 0.32558858 b= 0.25476432\n",
      "Epoch: 0836 loss= 0.095349938 w= 0.32549545 b= 0.2554344\n",
      "Epoch: 0837 loss= 0.095304817 w= 0.32540235 b= 0.25610363\n",
      "Epoch: 0838 loss= 0.095259815 w= 0.32530946 b= 0.256772\n",
      "Epoch: 0839 loss= 0.095214918 w= 0.32521665 b= 0.2574396\n",
      "Epoch: 0840 loss= 0.095170155 w= 0.32512397 b= 0.2581063\n",
      "Epoch: 0841 loss= 0.095125481 w= 0.3250314 b= 0.25877222\n",
      "Epoch: 0842 loss= 0.095080920 w= 0.32493892 b= 0.25943732\n",
      "Epoch: 0843 loss= 0.095036454 w= 0.3248466 b= 0.26010165\n",
      "Epoch: 0844 loss= 0.094992116 w= 0.32475436 b= 0.26076517\n",
      "Epoch: 0845 loss= 0.094947889 w= 0.3246622 b= 0.2614279\n",
      "Epoch: 0846 loss= 0.094903752 w= 0.32457024 b= 0.2620897\n",
      "Epoch: 0847 loss= 0.094859734 w= 0.32447833 b= 0.26275074\n",
      "Epoch: 0848 loss= 0.094815813 w= 0.3243866 b= 0.26341102\n",
      "Epoch: 0849 loss= 0.094772026 w= 0.32429495 b= 0.26407045\n",
      "Epoch: 0850 loss= 0.094728336 w= 0.3242034 b= 0.26472908\n",
      "Epoch: 0851 loss= 0.094684750 w= 0.32411194 b= 0.26538685\n",
      "Epoch: 0852 loss= 0.094641276 w= 0.32402062 b= 0.26604384\n",
      "Epoch: 0853 loss= 0.094597891 w= 0.3239294 b= 0.2667\n",
      "Epoch: 0854 loss= 0.094554625 w= 0.32383826 b= 0.2673554\n",
      "Epoch: 0855 loss= 0.094511472 w= 0.32374728 b= 0.26801008\n",
      "Epoch: 0856 loss= 0.094468407 w= 0.3236564 b= 0.26866382\n",
      "Epoch: 0857 loss= 0.094425477 w= 0.32356563 b= 0.2693168\n",
      "Epoch: 0858 loss= 0.094382629 w= 0.32347497 b= 0.269969\n",
      "Epoch: 0859 loss= 0.094339870 w= 0.32338443 b= 0.2706204\n",
      "Epoch: 0860 loss= 0.094297245 w= 0.32329404 b= 0.271271\n",
      "Epoch: 0861 loss= 0.094254717 w= 0.3232037 b= 0.27192077\n",
      "Epoch: 0862 loss= 0.094212286 w= 0.32311347 b= 0.27256975\n",
      "Epoch: 0863 loss= 0.094169967 w= 0.32302335 b= 0.27321795\n",
      "Epoch: 0864 loss= 0.094127752 w= 0.32293338 b= 0.27386537\n",
      "Epoch: 0865 loss= 0.094085641 w= 0.32284352 b= 0.274512\n",
      "Epoch: 0866 loss= 0.094043620 w= 0.32275376 b= 0.27515784\n",
      "Epoch: 0867 loss= 0.094001718 w= 0.32266414 b= 0.27580288\n",
      "Epoch: 0868 loss= 0.093959890 w= 0.32257456 b= 0.27644715\n",
      "Epoch: 0869 loss= 0.093918212 w= 0.32248512 b= 0.2770906\n",
      "Epoch: 0870 loss= 0.093876600 w= 0.3223957 b= 0.2777333\n",
      "Epoch: 0871 loss= 0.093835123 w= 0.32230645 b= 0.27837518\n",
      "Epoch: 0872 loss= 0.093793727 w= 0.32221735 b= 0.27901623\n",
      "Epoch: 0873 loss= 0.093752421 w= 0.32212836 b= 0.27965656\n",
      "Epoch: 0874 loss= 0.093711220 w= 0.32203943 b= 0.28029618\n",
      "Epoch: 0875 loss= 0.093670145 w= 0.32195067 b= 0.28093484\n",
      "Epoch: 0876 loss= 0.093629159 w= 0.32186195 b= 0.28157273\n",
      "Epoch: 0877 loss= 0.093588263 w= 0.32177338 b= 0.28220993\n",
      "Epoch: 0878 loss= 0.093547471 w= 0.3216849 b= 0.28284633\n",
      "Epoch: 0879 loss= 0.093506768 w= 0.3215966 b= 0.28348196\n",
      "Epoch: 0880 loss= 0.093466192 w= 0.32150835 b= 0.28411677\n",
      "Epoch: 0881 loss= 0.093425691 w= 0.32142025 b= 0.28475085\n",
      "Epoch: 0882 loss= 0.093385294 w= 0.32133222 b= 0.28538412\n",
      "Epoch: 0883 loss= 0.093345009 w= 0.3212443 b= 0.28601667\n",
      "Epoch: 0884 loss= 0.093304805 w= 0.3211565 b= 0.2866484\n",
      "Epoch: 0885 loss= 0.093264729 w= 0.3210688 b= 0.2872793\n",
      "Epoch: 0886 loss= 0.093224727 w= 0.3209812 b= 0.28790945\n",
      "Epoch: 0887 loss= 0.093184806 w= 0.32089373 b= 0.2885389\n",
      "Epoch: 0888 loss= 0.093145028 w= 0.32080635 b= 0.28916755\n",
      "Epoch: 0889 loss= 0.093105316 w= 0.32071912 b= 0.28979543\n",
      "Epoch: 0890 loss= 0.093065716 w= 0.3206319 b= 0.29042247\n",
      "Epoch: 0891 loss= 0.093026213 w= 0.32054478 b= 0.2910488\n",
      "Epoch: 0892 loss= 0.092986792 w= 0.32045788 b= 0.29167435\n",
      "Epoch: 0893 loss= 0.092947491 w= 0.32037097 b= 0.29229915\n",
      "Epoch: 0894 loss= 0.092908263 w= 0.32028425 b= 0.2929232\n",
      "Epoch: 0895 loss= 0.092869148 w= 0.32019755 b= 0.2935465\n",
      "Epoch: 0896 loss= 0.092830129 w= 0.32011107 b= 0.29416898\n",
      "Epoch: 0897 loss= 0.092791185 w= 0.32002464 b= 0.29479071\n",
      "Epoch: 0898 loss= 0.092752352 w= 0.31993833 b= 0.2954117\n",
      "Epoch: 0899 loss= 0.092713624 w= 0.31985208 b= 0.29603192\n",
      "Epoch: 0900 loss= 0.092674948 w= 0.31976599 b= 0.29665148\n",
      "Epoch: 0901 loss= 0.092636406 w= 0.31967998 b= 0.2972702\n",
      "Epoch: 0902 loss= 0.092597947 w= 0.31959406 b= 0.29788813\n",
      "Epoch: 0903 loss= 0.092559591 w= 0.31950822 b= 0.29850537\n",
      "Epoch: 0904 loss= 0.092521340 w= 0.31942254 b= 0.2991218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0905 loss= 0.092483170 w= 0.31933695 b= 0.29973745\n",
      "Epoch: 0906 loss= 0.092445076 w= 0.3192515 b= 0.30035236\n",
      "Epoch: 0907 loss= 0.092407092 w= 0.31916615 b= 0.30096653\n",
      "Epoch: 0908 loss= 0.092369206 w= 0.31908092 b= 0.30157995\n",
      "Epoch: 0909 loss= 0.092331417 w= 0.31899574 b= 0.30219263\n",
      "Epoch: 0910 loss= 0.092293687 w= 0.31891063 b= 0.30280456\n",
      "Epoch: 0911 loss= 0.092256084 w= 0.31882566 b= 0.3034157\n",
      "Epoch: 0912 loss= 0.092218570 w= 0.31874081 b= 0.30402607\n",
      "Epoch: 0913 loss= 0.092181139 w= 0.31865606 b= 0.30463576\n",
      "Epoch: 0914 loss= 0.092143804 w= 0.3185714 b= 0.3052447\n",
      "Epoch: 0915 loss= 0.092106543 w= 0.31848684 b= 0.3058529\n",
      "Epoch: 0916 loss= 0.092069410 w= 0.31840244 b= 0.30646032\n",
      "Epoch: 0917 loss= 0.092032306 w= 0.3183181 b= 0.307067\n",
      "Epoch: 0918 loss= 0.091995336 w= 0.31823394 b= 0.30767286\n",
      "Epoch: 0919 loss= 0.091958478 w= 0.3181498 b= 0.30827805\n",
      "Epoch: 0920 loss= 0.091921687 w= 0.31806576 b= 0.3088825\n",
      "Epoch: 0921 loss= 0.091884971 w= 0.31798184 b= 0.30948627\n",
      "Epoch: 0922 loss= 0.091848366 w= 0.317898 b= 0.3100892\n",
      "Epoch: 0923 loss= 0.091811843 w= 0.31781435 b= 0.31069145\n",
      "Epoch: 0924 loss= 0.091775410 w= 0.3177307 b= 0.31129298\n",
      "Epoch: 0925 loss= 0.091739058 w= 0.31764722 b= 0.31189376\n",
      "Epoch: 0926 loss= 0.091702819 w= 0.31756377 b= 0.31249377\n",
      "Epoch: 0927 loss= 0.091666646 w= 0.31748044 b= 0.31309307\n",
      "Epoch: 0928 loss= 0.091630585 w= 0.31739724 b= 0.3136916\n",
      "Epoch: 0929 loss= 0.091594592 w= 0.31731415 b= 0.31428936\n",
      "Epoch: 0930 loss= 0.091558695 w= 0.31723118 b= 0.31488642\n",
      "Epoch: 0931 loss= 0.091522880 w= 0.3171483 b= 0.31548277\n",
      "Epoch: 0932 loss= 0.091487154 w= 0.3170655 b= 0.31607836\n",
      "Epoch: 0933 loss= 0.091451533 w= 0.3169828 b= 0.31667325\n",
      "Epoch: 0934 loss= 0.091415994 w= 0.3169002 b= 0.3172674\n",
      "Epoch: 0935 loss= 0.091380544 w= 0.31681767 b= 0.31786078\n",
      "Epoch: 0936 loss= 0.091345154 w= 0.31673527 b= 0.3184535\n",
      "Epoch: 0937 loss= 0.091309883 w= 0.31665298 b= 0.31904542\n",
      "Epoch: 0938 loss= 0.091274671 w= 0.31657085 b= 0.31963667\n",
      "Epoch: 0939 loss= 0.091239572 w= 0.31648877 b= 0.32022712\n",
      "Epoch: 0940 loss= 0.091204531 w= 0.31640682 b= 0.32081693\n",
      "Epoch: 0941 loss= 0.091169581 w= 0.31632495 b= 0.32140595\n",
      "Epoch: 0942 loss= 0.091134742 w= 0.31624314 b= 0.3219943\n",
      "Epoch: 0943 loss= 0.091099977 w= 0.31616145 b= 0.32258192\n",
      "Epoch: 0944 loss= 0.091065280 w= 0.31607988 b= 0.3231688\n",
      "Epoch: 0945 loss= 0.091030680 w= 0.3159984 b= 0.323755\n",
      "Epoch: 0946 loss= 0.090996176 w= 0.31591704 b= 0.3243405\n",
      "Epoch: 0947 loss= 0.090961754 w= 0.31583577 b= 0.32492518\n",
      "Epoch: 0948 loss= 0.090927377 w= 0.3157546 b= 0.32550928\n",
      "Epoch: 0949 loss= 0.090893127 w= 0.31567347 b= 0.32609263\n",
      "Epoch: 0950 loss= 0.090858959 w= 0.3155925 b= 0.32667524\n",
      "Epoch: 0951 loss= 0.090824872 w= 0.31551155 b= 0.32725716\n",
      "Epoch: 0952 loss= 0.090790853 w= 0.31543076 b= 0.3278383\n",
      "Epoch: 0953 loss= 0.090756923 w= 0.31535012 b= 0.32841873\n",
      "Epoch: 0954 loss= 0.090723090 w= 0.31526953 b= 0.32899854\n",
      "Epoch: 0955 loss= 0.090689339 w= 0.315189 b= 0.3295776\n",
      "Epoch: 0956 loss= 0.090655640 w= 0.3151086 b= 0.33015597\n",
      "Epoch: 0957 loss= 0.090622067 w= 0.3150283 b= 0.33073357\n",
      "Epoch: 0958 loss= 0.090588547 w= 0.31494814 b= 0.33131048\n",
      "Epoch: 0959 loss= 0.090555131 w= 0.31486806 b= 0.3318867\n",
      "Epoch: 0960 loss= 0.090521775 w= 0.31478807 b= 0.33246222\n",
      "Epoch: 0961 loss= 0.090488501 w= 0.31470814 b= 0.33303705\n",
      "Epoch: 0962 loss= 0.090455331 w= 0.3146283 b= 0.3336111\n",
      "Epoch: 0963 loss= 0.090422228 w= 0.31454858 b= 0.33418447\n",
      "Epoch: 0964 loss= 0.090389200 w= 0.314469 b= 0.33475712\n",
      "Epoch: 0965 loss= 0.090356238 w= 0.3143895 b= 0.33532917\n",
      "Epoch: 0966 loss= 0.090323396 w= 0.31431007 b= 0.33590043\n",
      "Epoch: 0967 loss= 0.090290628 w= 0.31423077 b= 0.336471\n",
      "Epoch: 0968 loss= 0.090257913 w= 0.3141516 b= 0.33704093\n",
      "Epoch: 0969 loss= 0.090225309 w= 0.31407243 b= 0.33761013\n",
      "Epoch: 0970 loss= 0.090192765 w= 0.31399342 b= 0.3381786\n",
      "Epoch: 0971 loss= 0.090160288 w= 0.31391454 b= 0.33874643\n",
      "Epoch: 0972 loss= 0.090127923 w= 0.3138357 b= 0.33931357\n",
      "Epoch: 0973 loss= 0.090095624 w= 0.31375694 b= 0.33988\n",
      "Epoch: 0974 loss= 0.090063401 w= 0.31367826 b= 0.34044567\n",
      "Epoch: 0975 loss= 0.090031266 w= 0.3135997 b= 0.3410107\n",
      "Epoch: 0976 loss= 0.089999191 w= 0.31352127 b= 0.34157497\n",
      "Epoch: 0977 loss= 0.089967236 w= 0.31344292 b= 0.3421386\n",
      "Epoch: 0978 loss= 0.089935318 w= 0.3133647 b= 0.34270155\n",
      "Epoch: 0979 loss= 0.089903504 w= 0.3132865 b= 0.34326375\n",
      "Epoch: 0980 loss= 0.089871749 w= 0.3132085 b= 0.3438253\n",
      "Epoch: 0981 loss= 0.089840084 w= 0.31313053 b= 0.34438616\n",
      "Epoch: 0982 loss= 0.089808494 w= 0.31305262 b= 0.34494632\n",
      "Epoch: 0983 loss= 0.089776978 w= 0.31297484 b= 0.3455059\n",
      "Epoch: 0984 loss= 0.089745551 w= 0.31289715 b= 0.34606466\n",
      "Epoch: 0985 loss= 0.089714170 w= 0.3128196 b= 0.34662277\n",
      "Epoch: 0986 loss= 0.089682907 w= 0.31274208 b= 0.34718022\n",
      "Epoch: 0987 loss= 0.089651696 w= 0.31266472 b= 0.34773698\n",
      "Epoch: 0988 loss= 0.089620553 w= 0.31258744 b= 0.3482931\n",
      "Epoch: 0989 loss= 0.089589484 w= 0.31251022 b= 0.34884846\n",
      "Epoch: 0990 loss= 0.089558542 w= 0.31243312 b= 0.3494032\n",
      "Epoch: 0991 loss= 0.089527629 w= 0.31235614 b= 0.34995723\n",
      "Epoch: 0992 loss= 0.089496799 w= 0.31227922 b= 0.35051057\n",
      "Epoch: 0993 loss= 0.089466043 w= 0.31220242 b= 0.35106325\n",
      "Epoch: 0994 loss= 0.089435361 w= 0.31212568 b= 0.3516153\n",
      "Epoch: 0995 loss= 0.089404777 w= 0.312049 b= 0.35216662\n",
      "Epoch: 0996 loss= 0.089374252 w= 0.31197244 b= 0.35271725\n",
      "Epoch: 0997 loss= 0.089343801 w= 0.31189597 b= 0.35326725\n",
      "Epoch: 0998 loss= 0.089313440 w= 0.31181967 b= 0.35381654\n",
      "Epoch: 0999 loss= 0.089283131 w= 0.31174338 b= 0.35436514\n",
      "Epoch: 1000 loss= 0.089252897 w= 0.31166723 b= 0.3549131\n",
      "Optimization Finished!\n",
      "Training loss= 0.0892529 w= 0.31166723 b= 0.3549131 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can now run the computational graph in a session\n",
    "training_epochs = 1000              # We will run our model 1000 times\n",
    "display_step = 1                 # Display the loss every 100 runs\n",
    "final_w, final_b = 0,0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (sample_x, sample_y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={x: sample_x, y: sample_y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c , summary= sess.run([loss, merged_summary_op], feed_dict={x: train_X, y:train_Y})\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(c), \\\n",
    "                \"w=\", sess.run(w), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_loss = sess.run(loss, feed_dict={x: train_X, y:train_Y})\n",
    "    print(\"Training loss=\", training_loss, \"w=\", sess.run(w), \"b=\", sess.run(b), '\\n')\n",
    "    final_w, final_b = sess.run(w), sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, final_w * train_X + final_b, label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logistic regression refers to a classifier that classifies an observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes, but the two terms are used interchangebly. We will look at an example where we want to classify handwritten digits into one of 10 classes: 0-9\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the input to lie between 0 and 1. This function is called the logistic function, giving the model its name.\n",
    "\n",
    "We can create a logistic regressor in the same way as we created a linear regression computational graph.\n",
    "\n",
    "We will use the MNIST database of <a href=http://yann.lecun.com/exdb/mnist/>handwritten digits</a> for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TensorFlow provides easy access to some sample data sets. We can access the mnist dataset a TensorFlow dataset that contains 60,000 training images and their corresponding labels as well as 10,000 testing images and their corresponding labels.\n",
    "- Each image is 28 pixels by 28 pixels\n",
    "- Each image represents a digit between 0 and 9\n",
    "- The labels are one-hot encoded => each label is a 1x10 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Logistic regression model specification:\n",
    "\\begin{equation*}\n",
    "y = g(\\Theta^{T} X) = g(\\sum_{i=0}^{n} \\theta_{i} x_{i}) \\text{ where } x_0 = 1\n",
    "\\end{equation*}\n",
    "- g is the <a href = https://en.wikipedia.org/wiki/Softmax_function>softmax function</a>, an extension of the logistic function to multiple dimensions\n",
    "    - It squashes all dimensions of a vector input to lie between 0-1\n",
    "    - Ensures that the sum of the magnitudes of each dimension is 1\n",
    "\n",
    "- Tensorflow provides an implementation of the softmax function, which we can use.\n",
    "\n",
    "- The logistic regression model is trained by minimizing a cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "5. Softmax : https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "6. Reduce_mean : https://www.tensorflow.org/api_docs/python/tf/reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Specifying the model\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]))\n",
    "b = tf.Variable(initial_value = tf.zeros([10]))\n",
    "\n",
    "\n",
    "# Construct model\n",
    "prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax\n",
    "\n",
    "#lamb = 0.01 #Uncomment this line to enable regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1) # + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     )# uncomment on the line above to enable regularization and delete the paranthesis on this line\n",
    "                      \n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This time, we will use Batch Gradient Descent to optimize our loss function. However, Tensorflow is agnostic to the difference between Stochastic and Batch gradient descent. The only difference is in how we have defined our input placeholders in our computational graph and what we pass to the feed_dict parameter when we run the optimizer in a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_loss += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The advantages of Tensorflow lie in its extensability and ease of writing complex machine learning models with comparitive simplicity. We can modify the above Logistic regression model into a regularized logistic regression model with the simple addition of 2 code fragments.  \n",
    "\n",
    "```python\n",
    "lamb = 0.01 #This is the hyperparameter that controls the strength of the regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)  + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     \n",
    "```\n",
    "We use an L2 regularizer by just applying TF's inbuilt L2 regularizer on the parameters of our models"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "101px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
