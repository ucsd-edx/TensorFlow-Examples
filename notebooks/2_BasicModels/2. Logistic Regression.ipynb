{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Models in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Machine learning models can be implemented in tensorflow using the lower level tensorflow primitives which are operations and tensors.\n",
    "\n",
    "Tensorflow also provides low level primitives to specify optimizers that can find the maxima or minima of a loss function.\n",
    "\n",
    "If a machine learning model can be reduced to linear algebraic operations, it can be implemented in tensorflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will see:\n",
    "\n",
    "1. Linear regression\n",
    "\n",
    "2. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the others are considered to be dependent variables. For our example, we want to relate the variable Y to the variable X using a linear regression model. \n",
    "\n",
    "Specification of the model:\n",
    "$y$ = $b$ + $w_1$$x_1$ + ... +  $w_p$$x_p$\n",
    "- $y$ is the regressed variable\n",
    "- $w$'s are the weights\n",
    "- $b$ is the bias term\n",
    "- $x$'s are the features used to model y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Some toy data\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First we will build the computational graph for linear regression based on the algebraic equation that the model is defined by. We will use two new TensorFlow concepts, placeholders and variables, to build our graph. \n",
    "\n",
    "Placeholders are entry points into the graph allowing for training data to be passed into the graph.\n",
    "\n",
    "Variables are used to represent parameters of the graph which need to retain their value between runs (iterations) while training in a session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "1. Placeholders: https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "2. Variables: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the computational graph for linear regression with 1 explanatory variable\n",
    "# p = 1\n",
    "\n",
    "# Input to the graph\n",
    "y = tf.placeholder(dtype = tf.float32) # Placeholders - https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "x = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "# Model parameters are defined using variables\n",
    "# Variables - https://www.tensorflow.org/programmers_guide/variables\n",
    "# Variables retain their value even outside the bounds of a session's run call\n",
    "w = tf.Variable(initial_value = rng.randn(), name = \"weight\") \n",
    "b = tf.Variable(initial_value = rng.randn(), name = \"bias\")\n",
    "\n",
    "# Connecting up the nodes in our linear model\n",
    "# y = b + Wx\n",
    "\n",
    "prediction = tf.add(b, tf.multiply(w, x))\n",
    "\n",
    "# prediction holds the tensor that is the output of the operation add which takes tensors b, and the output of the multiply operation between the weight w, and the input x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model is complete, but our computational graph is not yet complete. To complete the computational graph, we need to define a loss function and an optimization strategy to allow for the training of the free variables, $b$ and $w$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Tensorflow provides various inbuilt optimizers that allow for the optimization of objective functions. These inbuilt optimizers are mostly directed toward neural network optimization, but a user can specify their own optimization functions by extending a base class. The base class provides access to various methods that calculate the gradients at all points in our computational graph. However, for most industrial projects the set of optimizers provided by TensorFlow are sufficient. \n",
    "\n",
    "To optimize a linear regressor, we will use the inbuilt Gradient Descent Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "3. reduce_sum operation: https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "4. Gradient descent optimizer: https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss for our model\n",
    "# Loss is the mean squared error between actual $y$ and predicted $y$\n",
    "\n",
    "loss = tf.reduce_sum( input_tensor = tf.pow(prediction-y, 2))/(2*n_samples)\n",
    "# reduce_sum is a function to compute the sum across dimensions of a tensor. In this case, the input tensor is a 1 x n_samples dimensional tensor of the prediction errors corresponding to the training samples  \n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "\n",
    "#Our previous definitions implicitly creates the relation between the loss and the variables w and b \n",
    "\n",
    "# We can use gradient descent to train our linear model\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now have a complete computational graph. Each run of the optimizer takes one sample of X and Y as input, makes a prediction. The optimizer updates the free variables in its loss function based on the prediction for that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We also need an operation to initialize our global variables (w and b)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 loss= 0.093090810 w= 0.3206871 b= 0.2900252\n",
      "Epoch: 0200 loss= 0.089578159 w= 0.31248203 b= 0.34905165\n",
      "Epoch: 0300 loss= 0.086830698 w= 0.30522373 b= 0.40126714\n",
      "Epoch: 0400 loss= 0.084682040 w= 0.2988035 b= 0.44745412\n",
      "Epoch: 0500 loss= 0.083001606 w= 0.29312393 b= 0.48831227\n",
      "Epoch: 0600 loss= 0.081687622 w= 0.28809997 b= 0.5244541\n",
      "Epoch: 0700 loss= 0.080660112 w= 0.28365585 b= 0.55642563\n",
      "Epoch: 0800 loss= 0.079856806 w= 0.27972442 b= 0.58470744\n",
      "Epoch: 0900 loss= 0.079228796 w= 0.27624667 b= 0.60972583\n",
      "Epoch: 1000 loss= 0.078737997 w= 0.27317083 b= 0.6318537\n",
      "Optimization Finished!\n",
      "Training loss= 0.078738 w= 0.27317083 b= 0.6318537 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can now run the computational graph in a session\n",
    "training_epochs = 1000              # We will run our model 1000 times\n",
    "display_step = 100                 # Display the loss every 100 runs\n",
    "final_w, final_b = 0,0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (sample_x, sample_y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={x: sample_x, y: sample_y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(loss, feed_dict={x: train_X, y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(c), \\\n",
    "                \"w=\", sess.run(w), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_loss = sess.run(loss, feed_dict={x: train_X, y:train_Y})\n",
    "    print(\"Training loss=\", training_loss, \"w=\", sess.run(w), \"b=\", sess.run(b), '\\n')\n",
    "    final_w, final_b = sess.run(w), sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2acc7419a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, final_w * train_X + final_b, label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logistic regression refers to a classifier that classifies an observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes, but the two terms are used interchangebly. We will look at an example where we want to classify handwritten digits into one of 10 classes: 0-9\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the input to lie between 0 and 1. This function is called the logistic function, giving the model its name.\n",
    "\n",
    "We can create a logistic regressor in the same way as we created a linear regression computational graph.\n",
    "\n",
    "We will use the MNIST database of <a href=http://yann.lecun.com/exdb/mnist/>handwritten digits</a> for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TensorFlow provides easy access to some sample data sets. We can access the mnist dataset a TensorFlow dataset that contains 60,000 training images and their corresponding labels as well as 10,000 testing images and their corresponding labels.\n",
    "- Each image is 28 pixels by 28 pixels\n",
    "- Each image represents a digit between 0 and 9\n",
    "- The labels are one-hot encoded => each label is a 1x10 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Logistic regression model specification:\n",
    "\\begin{equation*}\n",
    "y = g(\\Theta^{T} X) = g(\\sum_{i=0}^{n} \\theta_{i} x_{i}) \\text{ where } x_0 = 1\n",
    "\\end{equation*}\n",
    "- g is the <a href = https://en.wikipedia.org/wiki/Softmax_function>softmax function</a>, an extension of the logistic function to multiple dimensions\n",
    "    - It squashes all dimensions of a vector input to lie between 0-1\n",
    "    - Ensures that the sum of the magnitudes of each dimension is 1\n",
    "\n",
    "- Tensorflow provides an implementation of the softmax function, which we can use.\n",
    "\n",
    "- The logistic regression model is trained by minimizing a cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "5. Softmax : https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "6. Reduce_mean : https://www.tensorflow.org/api_docs/python/tf/reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Specifying the model\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]))\n",
    "b = tf.Variable(initial_value = tf.zeros([10]))\n",
    "\n",
    "\n",
    "# Construct model\n",
    "prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax\n",
    "\n",
    "#lamb = 0.01 #Uncomment this line to enable regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1) # + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     )# uncomment on the line above to enable regularization and delete the paranthesis on this line\n",
    "                      \n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This time, we will use Batch Gradient Descent to optimize our loss function. However, Tensorflow is agnostic to the difference between Stochastic and Batch gradient descent. The only difference is in how we have defined our input placeholders in our computational graph and what we pass to the feed_dict parameter when we run the optimizer in a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 loss= 0.465386575\n",
      "Epoch: 0010 loss= 0.391811254\n",
      "Epoch: 0015 loss= 0.363482976\n",
      "Epoch: 0020 loss= 0.345590337\n",
      "Epoch: 0025 loss= 0.332802501\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8883333\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_loss += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The advantages of Tensorflow lie in its extensability and ease of writing complex machine learning models with comparitive simplicity. We can modify the above Logistic regression model into a regularized logistic regression model with the simple addition of 2 code fragments.  \n",
    "\n",
    "```python\n",
    "lamb = 0.01 #This is the hyperparameter that controls the strength of the regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)  + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     \n",
    "```\n",
    "We use an L2 regularizer by just applying TF's inbuilt L2 regularizer on the parameters of our models"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "101px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
